[2024-11-20T09:44:50.453+0700] {processor.py:186} INFO - Started process (PID=79332) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:44:50.465+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T09:44:50.468+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:44:50.467+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:44:50.595+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:44:50.593+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 12, in <module>
    from scrapping_twitter import scrape_twitter # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ModuleNotFoundError: No module named 'scrapping_twitter'
[2024-11-20T09:44:50.598+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:44:50.816+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.444 seconds
[2024-11-20T09:45:21.307+0700] {processor.py:186} INFO - Started process (PID=79615) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:45:21.309+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T09:45:21.314+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:45:21.313+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:45:21.338+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:45:21.335+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 12, in <module>
    from scrapping_twitter import scrape_twitter # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ModuleNotFoundError: No module named 'scrapping_twitter'
[2024-11-20T09:45:21.339+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:45:21.434+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.143 seconds
[2024-11-20T09:45:51.817+0700] {processor.py:186} INFO - Started process (PID=79917) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:45:51.842+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T09:45:51.848+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:45:51.848+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:45:51.931+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:45:51.930+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 12, in <module>
    from scrapping_twitter import scrape_twitter # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ModuleNotFoundError: No module named 'scrapping_twitter'
[2024-11-20T09:45:51.932+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:45:52.048+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.341 seconds
[2024-11-20T09:46:22.637+0700] {processor.py:186} INFO - Started process (PID=80225) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:46:22.642+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T09:46:22.646+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:46:22.645+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:46:22.673+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:46:22.670+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 12, in <module>
    from scrapping_twitter import scrape_twitter # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ModuleNotFoundError: No module named 'scrapping_twitter'
[2024-11-20T09:46:22.674+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:46:22.732+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.118 seconds
[2024-11-20T09:46:53.164+0700] {processor.py:186} INFO - Started process (PID=80516) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:46:53.169+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T09:46:53.177+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:46:53.175+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:46:53.239+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:46:53.233+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 12, in <module>
    from scrapping_twitter import scrape_twitter # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ModuleNotFoundError: No module named 'scrapping_twitter'
[2024-11-20T09:46:53.242+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:46:53.420+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.307 seconds
[2024-11-20T09:47:24.066+0700] {processor.py:186} INFO - Started process (PID=80821) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:47:24.072+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T09:47:24.076+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:47:24.075+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:47:24.105+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:47:24.102+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 12, in <module>
    from scrapping_twitter import scrape_twitter # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ModuleNotFoundError: No module named 'scrapping_twitter'
[2024-11-20T09:47:24.106+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:47:24.166+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.218 seconds
[2024-11-20T09:47:54.994+0700] {processor.py:186} INFO - Started process (PID=81173) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:47:55.001+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T09:47:55.006+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:47:55.004+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:47:55.041+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:47:55.038+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 12, in <module>
    from scrapping_twitter import scrape_twitter # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ModuleNotFoundError: No module named 'scrapping_twitter'
[2024-11-20T09:47:55.043+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:47:55.125+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.176 seconds
[2024-11-20T09:48:25.299+0700] {processor.py:186} INFO - Started process (PID=81468) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:48:25.305+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T09:48:25.312+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:48:25.310+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:48:25.361+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:48:25.358+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 12, in <module>
    from scrapping_twitter import scrape_twitter # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ModuleNotFoundError: No module named 'scrapping_twitter'
[2024-11-20T09:48:25.362+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:48:25.448+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.186 seconds
[2024-11-20T09:48:56.331+0700] {processor.py:186} INFO - Started process (PID=81775) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:48:56.355+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T09:48:56.366+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:48:56.361+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:48:56.418+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:48:56.413+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 12, in <module>
    from scrapping_twitter import scrape_twitter # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ModuleNotFoundError: No module named 'scrapping_twitter'
[2024-11-20T09:48:56.420+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:48:56.528+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.293 seconds
[2024-11-20T09:49:26.813+0700] {processor.py:186} INFO - Started process (PID=82083) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:49:26.816+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T09:49:26.820+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:49:26.819+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:49:26.847+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:49:26.844+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 12, in <module>
    from scrapping_twitter import scrape_twitter # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ModuleNotFoundError: No module named 'scrapping_twitter'
[2024-11-20T09:49:26.848+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:49:26.930+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.157 seconds
[2024-11-20T09:49:57.745+0700] {processor.py:186} INFO - Started process (PID=82381) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:49:57.752+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T09:49:57.757+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:49:57.756+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:49:57.794+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:49:57.791+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 12, in <module>
    from scrapping_twitter import scrape_twitter # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ModuleNotFoundError: No module named 'scrapping_twitter'
[2024-11-20T09:49:57.796+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:49:57.895+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.177 seconds
[2024-11-20T09:50:28.607+0700] {processor.py:186} INFO - Started process (PID=82693) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:50:28.614+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T09:50:28.626+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:50:28.624+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:50:28.675+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:50:28.671+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 12, in <module>
    from scrapping_twitter import scrape_twitter # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ModuleNotFoundError: No module named 'scrapping_twitter'
[2024-11-20T09:50:28.676+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:50:28.791+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.242 seconds
[2024-11-20T09:50:34.348+0700] {processor.py:186} INFO - Started process (PID=82765) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:50:34.352+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T09:50:34.372+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:50:34.365+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:50:34.386+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:50:34.382+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1129, in get_code
  File "<frozen importlib._bootstrap_external>", line 1059, in source_to_code
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 13
    import scrape_twitter from .../notebooks/scrapping-twitter.py
                          ^^^^
SyntaxError: invalid syntax
[2024-11-20T09:50:34.387+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:50:34.532+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.272 seconds
[2024-11-20T09:50:57.444+0700] {processor.py:186} INFO - Started process (PID=82984) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:50:57.446+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T09:50:57.450+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:50:57.449+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:50:57.511+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:50:57.506+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 14, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/../../../notebooks/scrapping-twitter.py'
[2024-11-20T09:50:57.513+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:50:57.647+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.222 seconds
[2024-11-20T09:50:59.839+0700] {processor.py:186} INFO - Started process (PID=83012) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:50:59.841+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T09:50:59.845+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:50:59.844+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:50:59.877+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:50:59.873+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 14, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/../../../notebooks/scrapping-twitter.py'
[2024-11-20T09:50:59.881+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:50:59.985+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.165 seconds
[2024-11-20T09:51:14.598+0700] {processor.py:186} INFO - Started process (PID=83200) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:51:14.601+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T09:51:14.606+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:51:14.605+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:51:14.630+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:51:14.627+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 14, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/../../../notebooks/scrapping-twitter.py'
[2024-11-20T09:51:14.632+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:51:14.717+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.138 seconds
[2024-11-20T09:51:45.207+0700] {processor.py:186} INFO - Started process (PID=83530) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:51:45.209+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T09:51:45.212+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:51:45.211+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:51:45.234+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:51:45.232+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 14, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/../../../notebooks/scrapping-twitter.py'
[2024-11-20T09:51:45.236+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:51:45.319+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.133 seconds
[2024-11-20T09:52:15.732+0700] {processor.py:186} INFO - Started process (PID=83907) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:52:15.734+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T09:52:15.736+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:52:15.736+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:52:15.759+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:52:15.756+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 14, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/../../../notebooks/scrapping-twitter.py'
[2024-11-20T09:52:15.760+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:52:15.826+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.111 seconds
[2024-11-20T09:52:46.058+0700] {processor.py:186} INFO - Started process (PID=84393) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:52:46.060+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T09:52:46.062+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:52:46.061+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:52:46.083+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:52:46.081+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 14, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/../../../notebooks/scrapping-twitter.py'
[2024-11-20T09:52:46.084+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:52:46.165+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.126 seconds
[2024-11-20T09:52:53.527+0700] {processor.py:186} INFO - Started process (PID=84541) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:52:53.532+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T09:52:53.543+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:52:53.542+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:52:53.594+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:52:53.578+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 6, in <module>
    from dotenv import load_dotenv
ModuleNotFoundError: No module named 'dotenv'
[2024-11-20T09:52:53.596+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:52:53.715+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.225 seconds
[2024-11-20T09:53:16.846+0700] {processor.py:186} INFO - Started process (PID=85055) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:53:16.848+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T09:53:16.851+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:53:16.850+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:53:16.881+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:53:16.878+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 14, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/../../../notebooks/scrapping-twitter.py'
[2024-11-20T09:53:16.883+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:53:16.961+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.134 seconds
[2024-11-20T09:53:23.928+0700] {processor.py:186} INFO - Started process (PID=85161) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:53:23.929+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T09:53:23.939+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:53:23.938+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:53:23.946+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:53:23.944+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 6, in <module>
    from dotenv import load_dotenv
ModuleNotFoundError: No module named 'dotenv'
[2024-11-20T09:53:23.947+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:53:24.012+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.097 seconds
[2024-11-20T09:53:50.840+0700] {processor.py:186} INFO - Started process (PID=85729) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:53:50.842+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T09:53:50.849+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:53:50.848+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:53:50.874+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:53:50.871+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 6, in <module>
    from dotenv import load_dotenv
ModuleNotFoundError: No module named 'dotenv'
[2024-11-20T09:53:50.876+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:53:50.950+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.127 seconds
[2024-11-20T09:54:21.751+0700] {processor.py:186} INFO - Started process (PID=86080) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:54:21.757+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T09:54:21.771+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:54:21.770+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:54:21.780+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:54:21.778+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 6, in <module>
    from dotenv import load_dotenv
ModuleNotFoundError: No module named 'dotenv'
[2024-11-20T09:54:21.782+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:54:21.881+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.174 seconds
[2024-11-20T09:54:52.296+0700] {processor.py:186} INFO - Started process (PID=86493) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:54:52.304+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T09:54:52.308+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:54:52.308+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:54:52.323+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:54:52.317+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 6, in <module>
    from dotenv import load_dotenv
ModuleNotFoundError: No module named 'dotenv'
[2024-11-20T09:54:52.324+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:54:52.431+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.177 seconds
[2024-11-20T09:55:22.849+0700] {processor.py:186} INFO - Started process (PID=87038) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:55:22.854+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T09:55:22.868+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:55:22.863+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:55:22.892+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:55:22.882+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 6, in <module>
    from dotenv import load_dotenv
ModuleNotFoundError: No module named 'dotenv'
[2024-11-20T09:55:22.894+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:55:23.005+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.195 seconds
[2024-11-20T09:55:53.564+0700] {processor.py:186} INFO - Started process (PID=87351) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:55:53.574+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T09:55:53.604+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:55:53.603+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:55:53.628+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:55:53.621+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 6, in <module>
    from dotenv import load_dotenv
ModuleNotFoundError: No module named 'dotenv'
[2024-11-20T09:55:53.629+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:55:53.894+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.381 seconds
[2024-11-20T09:56:24.220+0700] {processor.py:186} INFO - Started process (PID=87832) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:56:24.231+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T09:56:24.239+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:56:24.238+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:56:24.249+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:56:24.247+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 6, in <module>
    from dotenv import load_dotenv
ModuleNotFoundError: No module named 'dotenv'
[2024-11-20T09:56:24.250+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:56:24.322+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.154 seconds
[2024-11-20T09:56:54.238+0700] {processor.py:186} INFO - Started process (PID=88312) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:56:54.241+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T09:56:54.248+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:56:54.247+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:56:54.308+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:56:54.303+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 14, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/../../../notebooks/scrapping-twitter.py'
[2024-11-20T09:56:54.310+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:56:54.482+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.272 seconds
[2024-11-20T09:57:25.059+0700] {processor.py:186} INFO - Started process (PID=88758) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:57:25.063+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T09:57:25.078+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:57:25.077+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:57:25.130+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:57:25.123+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 14, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/../../../notebooks/scrapping-twitter.py'
[2024-11-20T09:57:25.133+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:57:25.321+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.290 seconds
[2024-11-20T09:57:55.829+0700] {processor.py:186} INFO - Started process (PID=89081) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:57:55.830+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T09:57:55.835+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:57:55.834+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:57:55.855+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:57:55.852+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 14, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/../../../notebooks/scrapping-twitter.py'
[2024-11-20T09:57:55.856+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:57:55.944+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.130 seconds
[2024-11-20T09:58:21.104+0700] {processor.py:186} INFO - Started process (PID=89348) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:58:21.133+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T09:58:21.146+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:58:21.143+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:58:21.208+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:58:21.202+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 14, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/notebooks/scrapping-twitter.py'
[2024-11-20T09:58:21.210+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:58:21.405+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.314 seconds
[2024-11-20T09:58:46.059+0700] {processor.py:186} INFO - Started process (PID=89688) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:58:46.067+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T09:58:46.091+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:58:46.090+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:58:46.147+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:58:46.140+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 14, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/notebooks/scrapping-twitter.py'
[2024-11-20T09:58:46.150+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:58:46.244+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.213 seconds
[2024-11-20T09:59:16.773+0700] {processor.py:186} INFO - Started process (PID=90126) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:59:16.802+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T09:59:16.829+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:59:16.820+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:59:16.869+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:59:16.866+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 14, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/notebooks/scrapping-twitter.py'
[2024-11-20T09:59:16.875+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:59:17.016+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.283 seconds
[2024-11-20T09:59:28.880+0700] {processor.py:186} INFO - Started process (PID=90297) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:59:28.882+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T09:59:28.887+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:59:28.886+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:59:28.911+0700] {logging_mixin.py:190} INFO - [2024-11-20T09:59:28.909+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 14, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/../../notebooks/scrapping-twitter.py", line 1, in <module>
    from selenium import webdriver
ModuleNotFoundError: No module named 'selenium'
[2024-11-20T09:59:28.912+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T09:59:28.982+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.125 seconds
[2024-11-20T10:00:05.018+0700] {processor.py:186} INFO - Started process (PID=90673) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:00:05.022+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:00:05.032+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:00:05.030+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:00:05.098+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:00:05.089+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 14, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/../../notebooks/scrapping-twitter.py", line 1, in <module>
    from selenium import webdriver
ModuleNotFoundError: No module named 'selenium'
[2024-11-20T10:00:05.100+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:00:05.231+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.239 seconds
[2024-11-20T10:00:35.926+0700] {processor.py:186} INFO - Started process (PID=91093) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:00:35.929+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:00:35.936+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:00:35.934+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:00:35.980+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:00:35.977+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 14, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/notebooks/scrapping-twitter.py'
[2024-11-20T10:00:35.982+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:00:36.170+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.289 seconds
[2024-11-20T10:01:06.694+0700] {processor.py:186} INFO - Started process (PID=91397) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:01:06.701+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:01:06.718+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:01:06.716+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:01:06.767+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:01:06.759+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 14, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/notebooks/scrapping-twitter.py'
[2024-11-20T10:01:06.770+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:01:06.955+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.303 seconds
[2024-11-20T10:01:37.027+0700] {processor.py:186} INFO - Started process (PID=91703) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:01:37.031+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:01:37.043+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:01:37.037+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:01:37.070+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:01:37.067+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 14, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/notebooks/scrapping-twitter.py'
[2024-11-20T10:01:37.071+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:01:37.180+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.170 seconds
[2024-11-20T10:02:07.510+0700] {processor.py:186} INFO - Started process (PID=91996) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:02:07.512+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:02:07.517+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:02:07.516+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:02:07.543+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:02:07.540+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 14, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/notebooks/scrapping-twitter.py'
[2024-11-20T10:02:07.545+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:02:07.631+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.133 seconds
[2024-11-20T10:02:38.016+0700] {processor.py:186} INFO - Started process (PID=92302) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:02:38.025+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:02:38.034+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:02:38.033+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:02:38.068+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:02:38.057+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 14, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/notebooks/scrapping-twitter.py'
[2024-11-20T10:02:38.073+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:02:38.186+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.192 seconds
[2024-11-20T10:03:00.246+0700] {processor.py:186} INFO - Started process (PID=92555) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:03:00.248+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:03:00.253+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:03:00.253+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:03:00.276+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:03:00.273+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 14, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py", line 1, in <module>
    from selenium import webdriver
ModuleNotFoundError: No module named 'selenium'
[2024-11-20T10:03:00.277+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:03:00.356+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.127 seconds
[2024-11-20T10:03:17.130+0700] {processor.py:186} INFO - Started process (PID=92728) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:03:17.138+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:03:17.165+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:03:17.156+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:03:17.246+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:03:17.231+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 14, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py", line 1, in <module>
    from selenium import webdriver
ModuleNotFoundError: No module named 'selenium'
[2024-11-20T10:03:17.251+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:03:17.356+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.293 seconds
[2024-11-20T10:03:47.972+0700] {processor.py:186} INFO - Started process (PID=93163) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:03:47.976+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:03:47.981+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:03:47.981+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:03:48.006+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:03:48.003+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 14, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py", line 1, in <module>
    from selenium import webdriver
ModuleNotFoundError: No module named 'selenium'
[2024-11-20T10:03:48.011+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:03:48.104+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.160 seconds
[2024-11-20T10:04:43.412+0700] {processor.py:186} INFO - Started process (PID=93789) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:04:43.419+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:04:43.449+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:04:43.447+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:04:43.634+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:04:43.631+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 14, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py", line 11, in <module>
    from bs4 import BeautifulSoup
ModuleNotFoundError: No module named 'bs4'
[2024-11-20T10:04:43.635+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:04:43.701+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.320 seconds
[2024-11-20T10:05:59.286+0700] {processor.py:186} INFO - Started process (PID=94785) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:05:59.289+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:05:59.296+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:05:59.295+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:05:59.333+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:05:59.328+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 14, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py", line 1, in <module>
    from selenium import webdriver
ModuleNotFoundError: No module named 'selenium'
[2024-11-20T10:05:59.334+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:05:59.413+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.143 seconds
[2024-11-20T10:06:30.069+0700] {processor.py:186} INFO - Started process (PID=95253) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:06:30.075+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:06:30.082+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:06:30.081+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:06:30.111+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:06:30.108+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 14, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py", line 1, in <module>
    from selenium import webdriver
ModuleNotFoundError: No module named 'selenium'
[2024-11-20T10:06:30.113+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:06:30.259+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.206 seconds
[2024-11-20T10:07:00.830+0700] {processor.py:186} INFO - Started process (PID=95710) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:07:00.835+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:07:00.843+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:07:00.842+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:07:00.887+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:07:00.883+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 14, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py", line 1, in <module>
    from selenium import webdriver
ModuleNotFoundError: No module named 'selenium'
[2024-11-20T10:07:00.890+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:07:01.004+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.196 seconds
[2024-11-20T10:07:31.444+0700] {processor.py:186} INFO - Started process (PID=96101) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:07:31.450+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:07:31.469+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:07:31.466+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:07:31.515+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:07:31.511+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 14, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py", line 1, in <module>
    from selenium import webdriver
ModuleNotFoundError: No module named 'selenium'
[2024-11-20T10:07:31.519+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:07:31.627+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.220 seconds
[2024-11-20T10:07:50.781+0700] {processor.py:186} INFO - Started process (PID=96457) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:07:50.784+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:07:50.796+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:07:50.794+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:07:50.867+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:07:50.857+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 14, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py", line 1, in <module>
    from selenium import webdriver
ModuleNotFoundError: No module named 'selenium'
[2024-11-20T10:07:50.870+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:07:51.043+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.290 seconds
[2024-11-20T10:08:21.262+0700] {processor.py:186} INFO - Started process (PID=96949) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:08:21.266+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:08:21.283+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:08:21.281+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:08:21.340+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:08:21.335+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 14, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py", line 1, in <module>
    from selenium import webdriver
ModuleNotFoundError: No module named 'selenium'
[2024-11-20T10:08:21.343+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:08:21.530+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.299 seconds
[2024-11-20T10:11:28.291+0700] {processor.py:186} INFO - Started process (PID=98960) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:11:28.298+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:11:28.316+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:11:28.314+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:11:28.867+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:11:29.200+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:11:29.199+0700] {override.py:1911} INFO - Created Permission View: can delete on DAG:scrape_twitter
[2024-11-20T10:11:29.539+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:11:29.538+0700] {override.py:1911} INFO - Created Permission View: can read on DAG:scrape_twitter
[2024-11-20T10:11:29.573+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:11:29.572+0700] {override.py:1911} INFO - Created Permission View: can edit on DAG:scrape_twitter
[2024-11-20T10:11:29.606+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:11:29.605+0700] {override.py:1911} INFO - Created Permission View: can delete on DAG Run:scrape_twitter
[2024-11-20T10:11:29.642+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:11:29.641+0700] {override.py:1911} INFO - Created Permission View: menu access on DAG Run:scrape_twitter
[2024-11-20T10:11:29.670+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:11:29.669+0700] {override.py:1911} INFO - Created Permission View: can read on DAG Run:scrape_twitter
[2024-11-20T10:11:29.696+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:11:29.696+0700] {override.py:1911} INFO - Created Permission View: can create on DAG Run:scrape_twitter
[2024-11-20T10:11:29.698+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:11:29.697+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:11:29.740+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:11:29.739+0700] {dag.py:3262} INFO - Creating ORM DAG for scrape_twitter
[2024-11-20T10:11:29.768+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:11:29.767+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 02:00:00+00:00, run_after=2024-11-20 03:00:00+00:00
[2024-11-20T10:11:29.803+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.548 seconds
[2024-11-20T10:12:00.083+0700] {processor.py:186} INFO - Started process (PID=99401) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:12:00.085+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:12:00.091+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:12:00.090+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:12:00.693+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:12:00.782+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:12:00.780+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:12:00.907+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:12:00.907+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 02:00:00+00:00, run_after=2024-11-20 03:00:00+00:00
[2024-11-20T10:12:00.937+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.877 seconds
[2024-11-20T10:12:31.354+0700] {processor.py:186} INFO - Started process (PID=99798) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:12:31.358+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:12:31.364+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:12:31.363+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:12:32.075+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:12:32.149+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:12:32.148+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:12:32.204+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:12:32.203+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 02:00:00+00:00, run_after=2024-11-20 03:00:00+00:00
[2024-11-20T10:12:32.248+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.915 seconds
[2024-11-20T10:13:02.961+0700] {processor.py:186} INFO - Started process (PID=100520) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:13:02.967+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:13:02.975+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:13:02.974+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:13:03.830+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:13:03.993+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:13:03.993+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:13:04.134+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:13:04.129+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:13:04.279+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.349 seconds
[2024-11-20T10:13:34.515+0700] {processor.py:186} INFO - Started process (PID=101070) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:13:34.532+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:13:34.540+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:13:34.539+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:13:35.639+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:13:35.912+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:13:35.911+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:13:36.175+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:13:36.165+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:13:36.429+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.947 seconds
[2024-11-20T10:14:06.948+0700] {processor.py:186} INFO - Started process (PID=101633) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:14:06.979+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:14:07.018+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:14:06.997+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:14:07.995+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:14:08.082+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:14:08.082+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:14:08.148+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:14:08.147+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:14:08.730+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.797 seconds
[2024-11-20T10:14:39.584+0700] {processor.py:186} INFO - Started process (PID=102233) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:14:39.586+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:14:39.594+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:14:39.593+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:14:40.470+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:14:40.546+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:14:40.545+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:14:40.598+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:14:40.598+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:14:40.649+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.115 seconds
[2024-11-20T10:15:10.867+0700] {processor.py:186} INFO - Started process (PID=102882) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:15:10.943+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:15:10.964+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:15:10.962+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:15:12.302+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:15:12.463+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:15:12.460+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:15:12.561+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:15:12.560+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:15:12.644+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.817 seconds
[2024-11-20T10:15:42.855+0700] {processor.py:186} INFO - Started process (PID=103570) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:15:42.860+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:15:42.871+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:15:42.870+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:15:43.834+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:15:43.923+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:15:43.922+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:15:43.988+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:15:43.988+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:15:44.046+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.207 seconds
[2024-11-20T10:16:15.101+0700] {processor.py:186} INFO - Started process (PID=103960) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:16:15.108+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:16:15.124+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:16:15.121+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:16:15.656+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:16:15.722+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:16:15.722+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:16:15.766+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:16:15.765+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:16:15.801+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.734 seconds
[2024-11-20T10:16:45.983+0700] {processor.py:186} INFO - Started process (PID=104337) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:16:45.987+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:16:45.994+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:16:45.993+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:16:46.721+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:16:46.803+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:16:46.802+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:16:46.900+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:16:46.899+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:16:46.983+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.021 seconds
[2024-11-20T10:17:17.774+0700] {processor.py:186} INFO - Started process (PID=104641) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:17:17.793+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:17:17.823+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:17:17.812+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:17:18.441+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:17:18.495+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:17:18.494+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:17:18.552+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:17:18.550+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:17:18.633+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.909 seconds
[2024-11-20T10:17:48.859+0700] {processor.py:186} INFO - Started process (PID=104953) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:17:48.864+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:17:48.875+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:17:48.873+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:17:49.454+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:17:49.528+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:17:49.528+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:17:49.582+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:17:49.581+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:17:49.638+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.810 seconds
[2024-11-20T10:18:19.751+0700] {processor.py:186} INFO - Started process (PID=105271) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:18:19.756+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:18:19.768+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:18:19.766+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:18:20.345+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:18:20.410+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:18:20.409+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:18:20.458+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:18:20.457+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:18:20.501+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.784 seconds
[2024-11-20T10:18:51.204+0700] {processor.py:186} INFO - Started process (PID=105578) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:18:51.225+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:18:51.234+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:18:51.233+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:18:51.869+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:18:51.949+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:18:51.949+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:18:52.017+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:18:52.017+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:18:52.058+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.914 seconds
[2024-11-20T10:19:22.365+0700] {processor.py:186} INFO - Started process (PID=106020) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:19:22.372+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:19:22.378+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:19:22.378+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:19:22.984+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:19:23.039+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:19:23.038+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:19:23.082+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:19:23.081+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:19:23.114+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.766 seconds
[2024-11-20T10:19:53.944+0700] {processor.py:186} INFO - Started process (PID=106352) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:19:53.961+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:19:53.976+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:19:53.976+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:19:54.537+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:19:54.601+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:19:54.600+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:19:54.649+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:19:54.648+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:19:54.691+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.780 seconds
[2024-11-20T10:20:25.097+0700] {processor.py:186} INFO - Started process (PID=106809) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:20:25.098+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:20:25.101+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:20:25.100+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:20:25.351+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:20:25.418+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:20:25.418+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:20:25.470+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:20:25.470+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:20:25.514+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.747 seconds
[2024-11-20T10:20:55.706+0700] {processor.py:186} INFO - Started process (PID=107136) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:20:55.715+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:20:55.729+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:20:55.725+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:20:56.586+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:20:56.712+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:20:56.712+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:20:56.777+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:20:56.777+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:20:56.821+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.158 seconds
[2024-11-20T10:21:27.150+0700] {processor.py:186} INFO - Started process (PID=107621) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:21:27.156+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:21:27.167+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:21:27.166+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:21:27.611+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:21:27.706+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:21:27.705+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:21:27.769+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:21:27.768+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:21:27.821+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.723 seconds
[2024-11-20T10:21:58.724+0700] {processor.py:186} INFO - Started process (PID=108034) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:21:58.730+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:21:58.740+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:21:58.738+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:21:59.298+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:21:59.384+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:21:59.384+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:21:59.438+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:21:59.438+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:21:59.488+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.801 seconds
[2024-11-20T10:22:30.250+0700] {processor.py:186} INFO - Started process (PID=108419) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:22:30.252+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:22:30.264+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:22:30.262+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:22:30.610+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:22:30.674+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:22:30.674+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:22:30.731+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:22:30.731+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:22:30.797+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.565 seconds
[2024-11-20T10:23:01.074+0700] {processor.py:186} INFO - Started process (PID=108766) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:23:01.080+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:23:01.086+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:23:01.085+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:23:01.435+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:23:01.499+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:23:01.498+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:23:01.561+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:23:01.560+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:23:01.607+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.562 seconds
[2024-11-20T10:23:31.910+0700] {processor.py:186} INFO - Started process (PID=109073) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:23:31.911+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:23:31.920+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:23:31.919+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:23:32.549+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:23:32.625+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:23:32.625+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:23:32.689+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:23:32.689+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:23:32.735+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.873 seconds
[2024-11-20T10:24:03.008+0700] {processor.py:186} INFO - Started process (PID=109400) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:24:03.015+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:24:03.025+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:24:03.023+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:24:03.476+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:24:03.559+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:24:03.559+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:24:03.711+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:24:03.711+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:24:03.760+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.769 seconds
[2024-11-20T10:24:33.849+0700] {processor.py:186} INFO - Started process (PID=109717) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:24:33.851+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:24:33.858+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:24:33.857+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:24:34.284+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:24:34.364+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:24:34.361+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:24:34.433+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:24:34.432+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:24:34.468+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.641 seconds
[2024-11-20T10:24:56.192+0700] {processor.py:186} INFO - Started process (PID=110032) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:24:56.195+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:24:56.203+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:24:56.202+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:24:56.620+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:24:56.707+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:24:56.706+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:24:56.820+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:24:56.819+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:24:56.910+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.738 seconds
[2024-11-20T10:25:27.122+0700] {processor.py:186} INFO - Started process (PID=110525) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:25:27.125+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:25:27.130+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:25:27.129+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:25:27.660+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:25:27.851+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:25:27.851+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:25:28.066+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:25:28.066+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:25:28.136+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.044 seconds
[2024-11-20T10:25:58.921+0700] {processor.py:186} INFO - Started process (PID=111303) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:25:58.926+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:25:58.942+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:25:58.940+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:25:59.353+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:25:59.541+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:25:59.541+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:25:59.702+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:25:59.701+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:25:59.801+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.925 seconds
[2024-11-20T10:26:30.511+0700] {processor.py:186} INFO - Started process (PID=111780) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:26:30.513+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:26:30.518+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:26:30.517+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:26:30.875+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:26:30.993+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:26:30.992+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:26:31.397+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:26:31.395+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:26:31.468+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.971 seconds
[2024-11-20T10:27:01.847+0700] {processor.py:186} INFO - Started process (PID=112178) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:27:01.850+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:27:01.855+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:27:01.855+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:27:02.782+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:27:03.001+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:27:03.001+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:27:03.071+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:27:03.071+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:27:03.127+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.299 seconds
[2024-11-20T10:27:33.529+0700] {processor.py:186} INFO - Started process (PID=112517) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:27:33.531+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:27:33.540+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:27:33.536+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:27:34.301+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:27:34.489+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:27:34.489+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:27:34.589+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:27:34.589+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:27:34.673+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.191 seconds
[2024-11-20T10:28:05.096+0700] {processor.py:186} INFO - Started process (PID=112924) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:28:05.098+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:28:05.104+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:28:05.104+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:28:06.470+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:28:06.560+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:28:06.559+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:28:06.616+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:28:06.615+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:28:06.663+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.588 seconds
[2024-11-20T10:28:37.515+0700] {processor.py:186} INFO - Started process (PID=113467) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:28:37.520+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:28:37.531+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:28:37.530+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:28:38.205+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:28:38.391+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:28:38.390+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:28:39.005+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:28:39.005+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:28:39.075+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.582 seconds
[2024-11-20T10:29:09.350+0700] {processor.py:186} INFO - Started process (PID=113989) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:29:09.352+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:29:09.362+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:29:09.359+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:29:10.282+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:29:10.352+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:29:10.352+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:29:10.413+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:29:10.412+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:29:10.458+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.146 seconds
[2024-11-20T10:29:40.636+0700] {processor.py:186} INFO - Started process (PID=114553) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:29:40.638+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:29:40.646+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:29:40.645+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:29:41.550+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:29:41.642+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:29:41.641+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:29:41.723+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:29:41.722+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:29:41.781+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.165 seconds
[2024-11-20T10:30:11.944+0700] {processor.py:186} INFO - Started process (PID=114896) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:30:11.946+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:30:11.951+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:30:11.950+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:30:12.563+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:30:12.622+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:30:12.622+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:30:12.694+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:30:12.694+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:30:12.725+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.805 seconds
[2024-11-20T10:30:43.028+0700] {processor.py:186} INFO - Started process (PID=115308) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:30:43.032+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:30:43.037+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:30:43.037+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:30:43.523+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:30:43.598+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:30:43.597+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:30:43.664+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:30:43.664+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:30:43.714+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.706 seconds
[2024-11-20T10:31:13.824+0700] {processor.py:186} INFO - Started process (PID=115827) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:31:13.830+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:31:13.839+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:31:13.838+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:31:14.707+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:31:14.839+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:31:14.838+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:31:14.904+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:31:14.903+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:31:14.965+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.160 seconds
[2024-11-20T10:31:45.426+0700] {processor.py:186} INFO - Started process (PID=116204) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:31:45.436+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:31:45.454+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:31:45.453+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:31:46.216+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:31:46.283+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:31:46.282+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:31:46.338+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:31:46.338+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:31:46.392+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.036 seconds
[2024-11-20T10:32:16.764+0700] {processor.py:186} INFO - Started process (PID=116514) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:32:16.766+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:32:16.777+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:32:16.772+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:32:17.853+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:32:17.988+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:32:17.987+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:32:18.056+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:32:18.055+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:32:18.119+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.368 seconds
[2024-11-20T10:33:04.077+0700] {processor.py:186} INFO - Started process (PID=117057) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:33:04.080+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:33:04.086+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:33:04.085+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:33:04.435+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:33:04.511+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:33:04.511+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:33:04.565+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:33:04.565+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:33:04.610+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.546 seconds
[2024-11-20T10:33:34.700+0700] {processor.py:186} INFO - Started process (PID=117435) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:33:34.702+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:33:34.706+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:33:34.706+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:33:34.935+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:33:34.991+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:33:34.991+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:33:35.038+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:33:35.037+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:33:35.080+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.392 seconds
[2024-11-20T10:34:05.212+0700] {processor.py:186} INFO - Started process (PID=118002) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:34:05.222+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:34:05.230+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:34:05.229+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:34:05.698+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:34:05.827+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:34:05.827+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:34:05.911+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:34:05.911+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:34:05.984+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.807 seconds
[2024-11-20T10:34:36.266+0700] {processor.py:186} INFO - Started process (PID=118710) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:34:36.270+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:34:36.276+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:34:36.275+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:34:36.618+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:34:36.711+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:34:36.711+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:34:37.129+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:34:37.128+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:34:37.209+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.962 seconds
[2024-11-20T10:35:08.073+0700] {processor.py:186} INFO - Started process (PID=119235) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:35:08.079+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:35:08.097+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:35:08.097+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:35:08.958+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:35:09.046+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:35:09.045+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:35:09.132+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:35:09.132+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:35:09.194+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.155 seconds
[2024-11-20T10:35:39.657+0700] {processor.py:186} INFO - Started process (PID=119617) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:35:39.659+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:35:39.664+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:35:39.663+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:35:40.483+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:35:40.547+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:35:40.546+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:35:40.605+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:35:40.605+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:35:40.677+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.033 seconds
[2024-11-20T10:36:11.444+0700] {processor.py:186} INFO - Started process (PID=119976) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:36:11.446+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:36:11.452+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:36:11.451+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:36:11.996+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:36:12.069+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:36:12.068+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:36:12.113+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:36:12.113+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:36:12.152+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.724 seconds
[2024-11-20T10:36:42.317+0700] {processor.py:186} INFO - Started process (PID=120540) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:36:42.326+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:36:42.338+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:36:42.336+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:36:42.917+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:36:42.997+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:36:42.996+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:36:43.051+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:36:43.051+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:36:43.105+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.824 seconds
[2024-11-20T10:37:13.237+0700] {processor.py:186} INFO - Started process (PID=120905) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:37:13.242+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:37:13.247+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:37:13.247+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:37:13.807+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:37:13.882+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:37:13.881+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:37:13.991+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:37:13.990+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:37:14.029+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.811 seconds
[2024-11-20T10:37:44.380+0700] {processor.py:186} INFO - Started process (PID=121660) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:37:44.383+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:37:44.390+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:37:44.388+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:37:44.676+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:37:45.095+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:37:45.094+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:37:45.146+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:37:45.146+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:37:45.209+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.844 seconds
[2024-11-20T10:38:16.191+0700] {processor.py:186} INFO - Started process (PID=123038) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:38:16.195+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:38:16.203+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:38:16.203+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:38:16.980+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:38:17.089+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:38:17.088+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:38:17.155+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:38:17.154+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:38:21.204+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 5.051 seconds
[2024-11-20T10:38:51.308+0700] {processor.py:186} INFO - Started process (PID=123887) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:38:51.315+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:38:51.321+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:38:51.321+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:38:52.263+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:38:52.430+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:38:52.425+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:38:52.504+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:38:52.503+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:38:52.567+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.282 seconds
[2024-11-20T10:39:26.488+0700] {processor.py:186} INFO - Started process (PID=124795) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:39:26.494+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:39:26.507+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:39:26.506+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:39:27.084+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:39:27.158+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:39:27.158+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:39:27.221+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:39:27.221+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:39:38.523+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 12.084 seconds
[2024-11-20T10:40:19.458+0700] {processor.py:186} INFO - Started process (PID=125936) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:40:19.461+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:40:19.467+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:40:19.466+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:40:19.831+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:40:19.927+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:40:19.926+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:40:20.016+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:40:20.016+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:40:20.077+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.634 seconds
[2024-11-20T10:40:50.167+0700] {processor.py:186} INFO - Started process (PID=126545) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:40:50.170+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:40:50.174+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:40:50.174+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:40:50.586+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:40:50.655+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:40:50.654+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:40:50.703+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:40:50.703+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:40:50.740+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.586 seconds
[2024-11-20T10:41:21.062+0700] {processor.py:186} INFO - Started process (PID=127693) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:41:21.065+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:41:21.074+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:41:21.072+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:41:21.457+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:41:21.547+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:41:21.547+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:41:21.658+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:41:21.658+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:41:21.720+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.709 seconds
[2024-11-20T10:41:51.984+0700] {processor.py:186} INFO - Started process (PID=128110) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:41:51.989+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:41:51.997+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:41:51.994+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:41:52.735+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:41:52.825+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:41:52.824+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:41:52.875+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:41:52.874+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:41:52.911+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.952 seconds
[2024-11-20T10:42:23.314+0700] {processor.py:186} INFO - Started process (PID=128607) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:42:23.318+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:42:23.325+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:42:23.324+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:42:24.555+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:42:24.703+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:42:24.703+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:42:24.805+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:42:24.805+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:42:24.881+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.588 seconds
[2024-11-20T10:42:55.118+0700] {processor.py:186} INFO - Started process (PID=129142) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:42:55.121+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:42:55.125+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:42:55.124+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:42:55.915+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:42:55.978+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:42:55.978+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:42:56.036+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:42:56.035+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:42:56.087+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.993 seconds
[2024-11-20T10:43:26.400+0700] {processor.py:186} INFO - Started process (PID=129473) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:43:26.403+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:43:26.410+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:43:26.409+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:43:27.076+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:43:27.148+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:43:27.147+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:43:27.214+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:43:27.213+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:43:27.251+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.882 seconds
[2024-11-20T10:43:57.670+0700] {processor.py:186} INFO - Started process (PID=129803) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:43:57.680+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:43:57.702+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:43:57.698+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:43:59.146+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:43:59.282+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:43:59.281+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:43:59.396+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:43:59.395+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:43:59.477+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.843 seconds
[2024-11-20T10:44:30.024+0700] {processor.py:186} INFO - Started process (PID=130285) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:44:30.027+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:44:30.034+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:44:30.033+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:44:30.674+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:44:30.731+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:44:30.731+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:44:30.782+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:44:30.782+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:44:30.820+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.812 seconds
[2024-11-20T10:45:01.409+0700] {processor.py:186} INFO - Started process (PID=130603) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:45:01.412+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:45:01.417+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:45:01.416+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:45:02.011+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:45:02.063+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:45:02.063+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:45:02.101+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:45:02.101+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:45:02.135+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.749 seconds
[2024-11-20T10:45:32.422+0700] {processor.py:186} INFO - Started process (PID=130946) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:45:32.426+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:45:32.432+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:45:32.431+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:45:33.045+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:45:33.100+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:45:33.099+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:45:33.150+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:45:33.149+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:45:33.205+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.803 seconds
[2024-11-20T10:46:03.684+0700] {processor.py:186} INFO - Started process (PID=131408) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:46:03.695+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:46:03.703+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:46:03.700+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:46:04.464+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:46:04.536+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:46:04.535+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:46:04.589+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:46:04.589+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:46:04.713+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.133 seconds
[2024-11-20T10:46:35.308+0700] {processor.py:186} INFO - Started process (PID=131960) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:46:35.310+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:46:35.316+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:46:35.315+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:46:35.903+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:46:35.972+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:46:35.972+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:46:36.036+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:46:36.035+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:46:36.071+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.778 seconds
[2024-11-20T10:47:06.267+0700] {processor.py:186} INFO - Started process (PID=132807) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:47:06.268+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:47:06.284+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:47:06.283+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:47:07.189+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:47:07.287+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:47:07.287+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:47:07.350+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:47:07.350+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:47:07.390+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.157 seconds
[2024-11-20T10:47:37.713+0700] {processor.py:186} INFO - Started process (PID=133147) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:47:37.716+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:47:37.721+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:47:37.720+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:47:38.288+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:47:38.349+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:47:38.349+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:47:38.401+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:47:38.401+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:47:38.443+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.743 seconds
[2024-11-20T10:48:08.541+0700] {processor.py:186} INFO - Started process (PID=133643) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:48:08.543+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:48:08.549+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:48:08.548+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:48:09.275+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:48:09.359+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:48:09.358+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:48:09.450+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:48:09.449+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:48:09.504+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.984 seconds
[2024-11-20T10:48:39.907+0700] {processor.py:186} INFO - Started process (PID=133978) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:48:39.910+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:48:39.920+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:48:39.918+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:48:40.636+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:48:40.717+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:48:40.717+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:48:40.770+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:48:40.770+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:48:40.820+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.937 seconds
[2024-11-20T10:49:11.478+0700] {processor.py:186} INFO - Started process (PID=134277) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:49:11.481+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:49:11.488+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:49:11.487+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:49:12.290+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:49:12.374+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:49:12.373+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:49:12.437+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:49:12.436+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:49:12.530+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.079 seconds
[2024-11-20T10:49:42.777+0700] {processor.py:186} INFO - Started process (PID=134583) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:49:42.780+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:49:42.785+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:49:42.784+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:49:43.131+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:49:43.204+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:49:43.203+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:49:43.265+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:49:43.265+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:49:43.312+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.557 seconds
[2024-11-20T10:50:13.405+0700] {processor.py:186} INFO - Started process (PID=134876) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:50:13.407+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:50:13.414+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:50:13.413+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:50:13.742+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:50:13.846+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:50:13.842+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:50:13.906+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:50:13.906+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:50:13.951+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.563 seconds
[2024-11-20T10:50:44.757+0700] {processor.py:186} INFO - Started process (PID=135219) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:50:44.759+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:50:44.766+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:50:44.765+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:50:45.123+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:50:45.194+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:50:45.194+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:50:45.252+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:50:45.251+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:50:45.309+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.578 seconds
[2024-11-20T10:51:15.804+0700] {processor.py:186} INFO - Started process (PID=135512) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:51:15.809+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:51:15.818+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:51:15.817+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:51:16.411+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:51:16.564+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:51:16.562+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:51:16.679+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:51:16.678+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:51:16.743+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.963 seconds
[2024-11-20T10:51:47.003+0700] {processor.py:186} INFO - Started process (PID=135818) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:51:47.007+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:51:47.014+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:51:47.013+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:51:47.384+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:51:47.460+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:51:47.459+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:51:47.520+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:51:47.519+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:51:47.586+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.603 seconds
[2024-11-20T10:52:17.877+0700] {processor.py:186} INFO - Started process (PID=136109) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:52:17.880+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:52:17.893+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:52:17.892+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:52:18.412+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:52:18.508+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:52:18.507+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:52:18.590+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:52:18.589+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:52:18.637+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.804 seconds
[2024-11-20T10:52:48.909+0700] {processor.py:186} INFO - Started process (PID=136419) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:52:48.921+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:52:48.936+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:52:48.931+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:52:49.405+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:52:49.484+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:52:49.483+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:52:49.550+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:52:49.550+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:52:49.611+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.723 seconds
[2024-11-20T10:53:19.748+0700] {processor.py:186} INFO - Started process (PID=136711) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:53:19.755+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:53:19.760+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:53:19.759+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:53:20.174+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:53:20.269+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:53:20.269+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:53:20.345+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:53:20.344+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:53:20.421+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.696 seconds
[2024-11-20T10:53:50.731+0700] {processor.py:186} INFO - Started process (PID=137097) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:53:50.736+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:53:50.743+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:53:50.742+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:53:51.090+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:53:51.159+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:53:51.159+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:53:51.213+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:53:51.212+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:53:51.291+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.577 seconds
[2024-11-20T10:54:21.513+0700] {processor.py:186} INFO - Started process (PID=137383) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:54:21.514+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:54:21.520+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:54:21.519+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:54:21.979+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:54:22.136+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:54:22.134+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:54:22.213+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:54:22.212+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:54:22.286+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.790 seconds
[2024-11-20T10:54:53.022+0700] {processor.py:186} INFO - Started process (PID=137690) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:54:53.029+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:54:53.038+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:54:53.037+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:54:53.447+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:54:53.529+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:54:53.528+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:54:53.590+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:54:53.590+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:54:53.664+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.673 seconds
[2024-11-20T10:55:24.175+0700] {processor.py:186} INFO - Started process (PID=138025) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:55:24.178+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:55:24.182+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:55:24.182+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:55:24.589+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:55:24.677+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:55:24.677+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:55:24.730+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:55:24.729+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:55:24.767+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.621 seconds
[2024-11-20T10:55:54.914+0700] {processor.py:186} INFO - Started process (PID=138362) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:55:54.918+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:55:54.927+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:55:54.925+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:55:55.391+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:55:55.479+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:55:55.478+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:55:55.536+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:55:55.535+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:55:55.581+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.694 seconds
[2024-11-20T10:56:25.818+0700] {processor.py:186} INFO - Started process (PID=138651) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:56:25.822+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:56:25.833+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:56:25.832+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:56:26.229+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:56:26.297+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:56:26.297+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:56:26.353+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:56:26.352+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:56:26.399+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.609 seconds
[2024-11-20T10:56:56.712+0700] {processor.py:186} INFO - Started process (PID=138956) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:56:56.715+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:56:56.719+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:56:56.718+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:56:57.094+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:56:57.192+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:56:57.190+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:56:57.264+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:56:57.262+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:56:57.315+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.630 seconds
[2024-11-20T10:57:27.426+0700] {processor.py:186} INFO - Started process (PID=139246) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:57:27.430+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:57:27.438+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:57:27.437+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:57:27.806+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:57:27.881+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:57:27.879+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:57:27.941+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:57:27.941+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:57:27.987+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.583 seconds
[2024-11-20T10:57:58.424+0700] {processor.py:186} INFO - Started process (PID=139576) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:57:58.427+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:57:58.436+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:57:58.434+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:57:58.854+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:57:58.955+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:57:58.954+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:57:59.027+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:57:59.026+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:57:59.078+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.676 seconds
[2024-11-20T10:58:29.213+0700] {processor.py:186} INFO - Started process (PID=139875) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:58:29.216+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:58:29.221+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:58:29.221+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:58:29.548+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:58:29.617+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:58:29.616+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:58:29.693+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:58:29.692+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:58:29.859+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.667 seconds
[2024-11-20T10:58:59.964+0700] {processor.py:186} INFO - Started process (PID=140172) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:58:59.969+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:58:59.975+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:58:59.974+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:59:00.382+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:59:00.471+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:59:00.470+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:59:00.548+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:59:00.548+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:59:00.606+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.657 seconds
[2024-11-20T10:59:30.925+0700] {processor.py:186} INFO - Started process (PID=140682) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:59:30.927+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T10:59:30.931+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:59:30.930+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:59:31.263+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T10:59:31.346+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:59:31.346+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T10:59:31.403+0700] {logging_mixin.py:190} INFO - [2024-11-20T10:59:31.403+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 03:00:00+00:00, run_after=2024-11-20 04:00:00+00:00
[2024-11-20T10:59:31.487+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.583 seconds
[2024-11-20T11:00:02.108+0700] {processor.py:186} INFO - Started process (PID=141103) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:00:02.111+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:00:02.118+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:00:02.116+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:00:02.830+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:00:03.013+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:00:03.012+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:00:03.143+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:00:03.142+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:00:03.209+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.151 seconds
[2024-11-20T11:00:33.809+0700] {processor.py:186} INFO - Started process (PID=142212) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:00:33.813+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:00:33.824+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:00:33.821+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:00:34.249+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:00:34.356+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:00:34.354+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:00:34.421+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:00:34.420+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:00:34.464+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.697 seconds
[2024-11-20T11:01:05.331+0700] {processor.py:186} INFO - Started process (PID=142534) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:01:05.341+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:01:05.353+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:01:05.351+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:01:05.809+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:01:05.975+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:01:05.974+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:01:06.136+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:01:06.136+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:01:06.271+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.986 seconds
[2024-11-20T11:01:36.475+0700] {processor.py:186} INFO - Started process (PID=143106) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:01:36.480+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:01:36.499+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:01:36.498+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:01:37.022+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:01:37.183+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:01:37.182+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:01:37.297+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:01:37.296+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:01:37.374+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.949 seconds
[2024-11-20T11:02:07.991+0700] {processor.py:186} INFO - Started process (PID=144054) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:02:07.993+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:02:07.998+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:02:07.997+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:02:08.954+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:02:09.133+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:02:09.133+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:02:09.242+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:02:09.239+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:02:09.336+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.391 seconds
[2024-11-20T11:02:39.994+0700] {processor.py:186} INFO - Started process (PID=144510) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:02:40.003+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:02:40.040+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:02:40.038+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:02:40.588+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:02:40.706+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:02:40.706+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:02:40.784+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:02:40.783+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:02:40.836+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.897 seconds
[2024-11-20T11:03:11.127+0700] {processor.py:186} INFO - Started process (PID=145073) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:03:11.136+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:03:11.154+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:03:11.152+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:03:11.956+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:03:12.055+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:03:12.054+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:03:12.137+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:03:12.137+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:03:12.205+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.139 seconds
[2024-11-20T11:03:42.774+0700] {processor.py:186} INFO - Started process (PID=145609) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:03:42.777+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:03:42.787+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:03:42.786+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:03:43.330+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:03:43.504+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:03:43.504+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:03:43.609+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:03:43.606+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:03:43.692+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.949 seconds
[2024-11-20T11:04:14.498+0700] {processor.py:186} INFO - Started process (PID=146143) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:04:14.500+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:04:14.508+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:04:14.508+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:04:14.798+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:04:14.942+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:04:14.941+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:04:15.060+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:04:15.059+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:04:15.146+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.662 seconds
[2024-11-20T11:04:45.329+0700] {processor.py:186} INFO - Started process (PID=146441) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:04:45.333+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:04:45.342+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:04:45.341+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:04:45.719+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:04:45.816+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:04:45.816+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:04:45.880+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:04:45.879+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:04:45.931+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.623 seconds
[2024-11-20T11:05:16.115+0700] {processor.py:186} INFO - Started process (PID=146740) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:05:16.118+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:05:16.134+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:05:16.133+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:05:16.544+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:05:16.623+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:05:16.623+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:05:16.707+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:05:16.707+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:05:16.771+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.688 seconds
[2024-11-20T11:05:46.884+0700] {processor.py:186} INFO - Started process (PID=147044) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:05:46.888+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:05:46.898+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:05:46.897+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:05:47.345+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:05:47.519+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:05:47.518+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:05:47.640+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:05:47.639+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:05:47.749+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.883 seconds
[2024-11-20T11:06:18.026+0700] {processor.py:186} INFO - Started process (PID=147333) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:06:18.031+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:06:18.045+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:06:18.043+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:06:18.559+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:06:18.641+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:06:18.640+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:06:18.778+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:06:18.777+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:06:18.883+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.900 seconds
[2024-11-20T11:06:49.088+0700] {processor.py:186} INFO - Started process (PID=148293) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:06:49.089+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:06:49.099+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:06:49.098+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:06:49.414+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:06:49.506+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:06:49.506+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:06:49.567+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:06:49.566+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:06:49.651+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.580 seconds
[2024-11-20T11:07:19.802+0700] {processor.py:186} INFO - Started process (PID=148596) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:07:19.806+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:07:19.816+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:07:19.816+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:07:20.415+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:07:20.600+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:07:20.600+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:07:20.699+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:07:20.699+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:07:20.779+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.007 seconds
[2024-11-20T11:07:51.231+0700] {processor.py:186} INFO - Started process (PID=149037) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:07:51.232+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:07:51.239+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:07:51.239+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:07:51.603+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:07:51.677+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:07:51.676+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:07:51.739+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:07:51.738+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:07:51.785+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.622 seconds
[2024-11-20T11:08:22.157+0700] {processor.py:186} INFO - Started process (PID=149972) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:08:22.166+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:08:22.180+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:08:22.179+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:08:23.013+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:08:23.121+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:08:23.121+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:08:23.217+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:08:23.217+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:08:23.299+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.241 seconds
[2024-11-20T11:34:00.426+0700] {processor.py:186} INFO - Started process (PID=150288) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:34:00.428+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:34:00.432+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:34:00.432+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:34:00.856+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:34:00.949+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:34:00.948+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:34:01.065+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:34:01.064+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:34:01.120+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.708 seconds
[2024-11-20T11:34:31.956+0700] {processor.py:186} INFO - Started process (PID=150714) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:34:31.959+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:34:31.969+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:34:31.965+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:34:32.364+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:34:32.440+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:34:32.439+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:34:32.538+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:34:32.538+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:34:32.637+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.717 seconds
[2024-11-20T11:35:03.562+0700] {processor.py:186} INFO - Started process (PID=151194) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:35:03.565+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:35:03.570+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:35:03.569+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:35:03.894+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:35:03.972+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:35:03.971+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:35:04.033+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:35:04.032+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:35:04.071+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.519 seconds
[2024-11-20T11:35:34.883+0700] {processor.py:186} INFO - Started process (PID=151649) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:35:34.885+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:35:34.890+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:35:34.889+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:35:35.295+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:35:35.364+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:35:35.363+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:35:35.421+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:35:35.421+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:35:35.457+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.589 seconds
[2024-11-20T11:36:05.555+0700] {processor.py:186} INFO - Started process (PID=151992) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:36:05.557+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:36:05.562+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:36:05.561+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:36:05.793+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:36:05.849+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:36:05.849+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:36:05.896+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:36:05.896+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:36:05.959+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.414 seconds
[2024-11-20T11:36:36.643+0700] {processor.py:186} INFO - Started process (PID=152313) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:36:36.650+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:36:36.653+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:36:36.652+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:36:36.859+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:36:36.908+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:36:36.907+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:36:36.961+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:36:36.961+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:36:37.041+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.408 seconds
[2024-11-20T11:37:07.774+0700] {processor.py:186} INFO - Started process (PID=152629) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:37:07.775+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:37:07.779+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:37:07.778+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:37:07.936+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:37:07.985+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:37:07.985+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:37:08.023+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:37:08.022+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:37:08.067+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.302 seconds
[2024-11-20T11:37:38.431+0700] {processor.py:186} INFO - Started process (PID=152929) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:37:38.443+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:37:38.464+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:37:38.446+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:37:38.828+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:37:38.882+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:37:38.882+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:37:38.925+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:37:38.924+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:37:38.974+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.558 seconds
[2024-11-20T11:38:09.324+0700] {processor.py:186} INFO - Started process (PID=153271) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:38:09.325+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:38:09.328+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:38:09.327+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:38:09.468+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:38:09.500+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:38:09.500+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:38:09.523+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:38:09.523+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:38:09.543+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.226 seconds
[2024-11-20T11:38:40.024+0700] {processor.py:186} INFO - Started process (PID=154329) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:38:40.027+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:38:40.039+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:38:40.032+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:38:40.364+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:38:40.442+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:38:40.442+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:38:40.514+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:38:40.513+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:38:40.558+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.567 seconds
[2024-11-20T11:39:10.741+0700] {processor.py:186} INFO - Started process (PID=154654) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:39:10.746+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:39:10.755+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:39:10.754+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:39:12.007+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:39:12.394+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:39:12.393+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:39:12.551+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:39:12.546+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:39:12.672+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.962 seconds
[2024-11-20T11:39:43.624+0700] {processor.py:186} INFO - Started process (PID=154969) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:39:43.634+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:39:43.681+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:39:43.675+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:39:46.673+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:39:47.234+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:39:47.225+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:39:47.735+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:39:47.734+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:39:48.057+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 4.583 seconds
[2024-11-20T11:40:18.820+0700] {processor.py:186} INFO - Started process (PID=155462) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:40:18.822+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:40:18.843+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:40:18.842+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:40:19.891+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:40:19.990+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:40:19.989+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:40:20.135+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:40:20.132+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:40:20.307+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.518 seconds
[2024-11-20T11:40:50.624+0700] {processor.py:186} INFO - Started process (PID=156022) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:40:50.627+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:40:50.639+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:40:50.639+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:40:50.901+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:40:50.971+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:40:50.971+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:40:51.022+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:40:51.021+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:40:51.083+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.507 seconds
[2024-11-20T11:41:21.142+0700] {processor.py:186} INFO - Started process (PID=156481) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:41:21.144+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:41:21.149+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:41:21.148+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:41:21.463+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:41:21.547+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:41:21.546+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:41:21.611+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:41:21.611+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:41:21.652+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.522 seconds
[2024-11-20T11:41:52.072+0700] {processor.py:186} INFO - Started process (PID=156776) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:41:52.073+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:41:52.078+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:41:52.077+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:41:52.438+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:41:52.620+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:41:52.618+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:41:52.848+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:41:52.846+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:41:52.939+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.901 seconds
[2024-11-20T11:42:23.371+0700] {processor.py:186} INFO - Started process (PID=157089) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:42:23.374+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:42:23.385+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:42:23.384+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:42:24.044+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:42:24.238+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:42:24.237+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:42:24.343+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:42:24.341+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:42:24.433+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.091 seconds
[2024-11-20T11:42:54.891+0700] {processor.py:186} INFO - Started process (PID=157391) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:42:54.901+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:42:54.926+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:42:54.923+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:42:55.630+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:42:55.765+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:42:55.764+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:42:55.892+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:42:55.892+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:42:55.965+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.103 seconds
[2024-11-20T11:43:26.038+0700] {processor.py:186} INFO - Started process (PID=157746) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:43:26.041+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:43:26.046+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:43:26.046+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:43:26.074+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:43:26.071+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 14, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1129, in get_code
  File "<frozen importlib._bootstrap_external>", line 1059, in source_to_code
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py", line 142
    print(f"Data successfully saved to CSV: {csv_file_name}")
IndentationError: unexpected indent
[2024-11-20T11:43:26.075+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:43:26.134+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.116 seconds
[2024-11-20T11:43:56.474+0700] {processor.py:186} INFO - Started process (PID=158070) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:43:56.477+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:43:56.485+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:43:56.485+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:43:56.831+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:43:56.941+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:43:56.940+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:43:57.055+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:43:57.055+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:43:57.109+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.652 seconds
[2024-11-20T11:44:27.351+0700] {processor.py:186} INFO - Started process (PID=158376) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:44:27.353+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:44:27.359+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:44:27.358+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:44:27.661+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:44:27.719+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:44:27.718+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:44:27.759+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:44:27.759+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:44:27.791+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.455 seconds
[2024-11-20T11:45:05.948+0700] {processor.py:186} INFO - Started process (PID=158837) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:45:05.951+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:45:05.989+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:45:05.980+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:45:06.901+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:45:07.044+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:45:07.044+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:45:07.232+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:45:07.227+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:45:07.396+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.520 seconds
[2024-11-20T11:45:37.651+0700] {processor.py:186} INFO - Started process (PID=160064) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:45:37.654+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:45:37.659+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:45:37.659+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:45:38.072+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:45:38.139+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:45:38.138+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:45:38.187+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:45:38.187+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:45:38.229+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.612 seconds
[2024-11-20T11:46:08.702+0700] {processor.py:186} INFO - Started process (PID=160624) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:46:08.707+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:46:08.738+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:46:08.730+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:46:09.549+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:46:09.688+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:46:09.688+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:46:09.776+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:46:09.776+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:46:09.860+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.192 seconds
[2024-11-20T11:46:40.085+0700] {processor.py:186} INFO - Started process (PID=161095) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:46:40.088+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:46:40.098+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:46:40.097+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:46:40.961+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:46:41.092+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:46:41.091+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:46:41.207+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:46:41.205+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:46:41.266+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.206 seconds
[2024-11-20T11:47:36.452+0700] {processor.py:186} INFO - Started process (PID=161577) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:47:36.457+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:47:36.503+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:47:36.497+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:47:39.267+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:47:39.708+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:47:39.701+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:47:40.269+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:47:40.261+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:47:40.464+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 4.078 seconds
[2024-11-20T11:48:11.717+0700] {processor.py:186} INFO - Started process (PID=162090) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:48:11.722+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:48:11.765+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:48:11.754+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:48:13.321+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:48:13.618+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:48:13.617+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:48:13.890+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:48:13.889+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:48:14.078+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 2.415 seconds
[2024-11-20T11:48:52.741+0700] {processor.py:186} INFO - Started process (PID=162472) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:48:52.744+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:48:52.752+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:48:52.749+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:48:53.100+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:48:53.185+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:48:53.184+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:48:53.243+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:48:53.242+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:48:59.297+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 6.588 seconds
[2024-11-20T11:49:41.935+0700] {processor.py:186} INFO - Started process (PID=162870) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:49:41.968+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:49:41.993+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:49:41.977+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:49:44.970+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:49:45.434+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:49:45.432+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:49:46.198+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:49:46.198+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:49:46.486+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 4.657 seconds
[2024-11-20T11:50:17.032+0700] {processor.py:186} INFO - Started process (PID=163207) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:50:17.040+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:50:17.060+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:50:17.059+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:50:17.415+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:50:17.479+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:50:17.479+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:50:17.533+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:50:17.533+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:50:17.754+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.737 seconds
[2024-11-20T11:50:47.867+0700] {processor.py:186} INFO - Started process (PID=163516) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:50:47.868+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:50:47.876+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:50:47.875+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:50:48.064+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:50:48.112+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:50:48.112+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:50:48.144+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:50:48.144+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:50:48.171+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.313 seconds
[2024-11-20T11:51:18.519+0700] {processor.py:186} INFO - Started process (PID=163847) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:51:18.520+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:51:18.523+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:51:18.523+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:51:18.675+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:51:18.709+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:51:18.708+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:51:18.735+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:51:18.735+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:51:18.772+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.263 seconds
[2024-11-20T11:51:49.133+0700] {processor.py:186} INFO - Started process (PID=164150) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:51:49.134+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:51:49.139+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:51:49.139+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:51:49.345+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:51:49.407+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:51:49.407+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:51:49.455+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:51:49.455+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:51:49.501+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.378 seconds
[2024-11-20T11:52:19.778+0700] {processor.py:186} INFO - Started process (PID=164560) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:52:19.782+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:52:19.791+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:52:19.788+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:52:20.684+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:52:20.774+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:52:20.774+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:52:20.846+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:52:20.845+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:52:20.901+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.144 seconds
[2024-11-20T11:52:51.047+0700] {processor.py:186} INFO - Started process (PID=164860) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:52:51.051+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:52:51.058+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:52:51.057+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:52:51.472+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:52:51.549+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:52:51.548+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:52:51.619+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:52:51.619+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:52:51.674+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.644 seconds
[2024-11-20T11:53:22.684+0700] {processor.py:186} INFO - Started process (PID=165238) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:53:22.686+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:53:22.690+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:53:22.689+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:53:22.901+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:53:22.967+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:53:22.967+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:53:23.046+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:53:23.046+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:53:23.094+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.422 seconds
[2024-11-20T11:53:53.658+0700] {processor.py:186} INFO - Started process (PID=165679) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:53:53.667+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:53:53.693+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:53:53.692+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:53:54.579+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:53:54.696+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:53:54.696+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:53:55.272+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:53:55.271+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:53:55.331+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.709 seconds
[2024-11-20T11:54:26.884+0700] {processor.py:186} INFO - Started process (PID=166161) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:54:26.890+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:54:26.903+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:54:26.902+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:54:30.233+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:54:30.541+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:54:30.538+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:54:30.752+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:54:30.751+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:54:31.012+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 4.188 seconds
[2024-11-20T11:55:02.100+0700] {processor.py:186} INFO - Started process (PID=166504) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:55:02.101+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:55:02.109+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:55:02.109+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:55:03.116+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:55:03.195+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:55:03.194+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:55:03.278+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:55:03.277+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:55:03.329+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.298 seconds
[2024-11-20T11:55:33.505+0700] {processor.py:186} INFO - Started process (PID=166865) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:55:33.508+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:55:33.512+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:55:33.511+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:55:34.150+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:55:34.258+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:55:34.257+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:55:34.328+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:55:34.328+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:55:34.408+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.921 seconds
[2024-11-20T11:56:05.022+0700] {processor.py:186} INFO - Started process (PID=167175) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:56:05.030+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:56:05.050+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:56:05.049+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:56:07.297+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:56:07.487+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:56:07.487+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:56:07.653+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:56:07.652+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:56:07.743+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 2.788 seconds
[2024-11-20T11:56:37.847+0700] {processor.py:186} INFO - Started process (PID=167500) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:56:37.848+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:56:37.850+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:56:37.850+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:56:38.152+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:56:38.200+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:56:38.200+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:56:38.223+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:56:38.223+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:56:38.247+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.406 seconds
[2024-11-20T11:57:09.133+0700] {processor.py:186} INFO - Started process (PID=167800) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:57:09.134+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:57:09.137+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:57:09.136+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:57:09.458+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:57:09.502+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:57:09.501+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:57:09.526+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:57:09.526+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:57:09.550+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.426 seconds
[2024-11-20T11:57:39.619+0700] {processor.py:186} INFO - Started process (PID=168095) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:57:39.621+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:57:39.624+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:57:39.624+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:57:40.095+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:57:40.147+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:57:40.147+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:57:40.195+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:57:40.194+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:57:40.229+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.626 seconds
[2024-11-20T11:58:10.290+0700] {processor.py:186} INFO - Started process (PID=168426) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:58:10.292+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:58:10.295+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:58:10.294+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:58:10.694+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:58:10.739+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:58:10.739+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:58:10.776+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:58:10.776+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:58:10.807+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.529 seconds
[2024-11-20T11:58:41.034+0700] {processor.py:186} INFO - Started process (PID=168720) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:58:41.035+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:58:41.039+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:58:41.038+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:58:41.768+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:58:41.906+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:58:41.905+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:58:41.974+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:58:41.974+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:58:42.009+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.987 seconds
[2024-11-20T11:59:12.103+0700] {processor.py:186} INFO - Started process (PID=169026) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:59:12.104+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:59:12.107+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:59:12.106+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:59:12.459+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:59:12.526+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:59:12.526+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:59:12.552+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:59:12.551+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:59:12.573+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.479 seconds
[2024-11-20T11:59:42.649+0700] {processor.py:186} INFO - Started process (PID=169308) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:59:42.653+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T11:59:42.660+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:59:42.659+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:59:43.359+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T11:59:43.429+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:59:43.429+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T11:59:43.486+0700] {logging_mixin.py:190} INFO - [2024-11-20T11:59:43.485+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 04:00:00+00:00, run_after=2024-11-20 05:00:00+00:00
[2024-11-20T11:59:43.530+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.898 seconds
[2024-11-20T12:00:13.732+0700] {processor.py:186} INFO - Started process (PID=169723) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:00:13.735+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:00:13.750+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:00:13.749+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:00:14.946+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:00:15.066+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:00:15.065+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:00:15.186+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:00:15.185+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:00:15.271+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.562 seconds
[2024-11-20T12:00:45.698+0700] {processor.py:186} INFO - Started process (PID=170678) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:00:45.703+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:00:45.709+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:00:45.708+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:00:46.780+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:00:46.858+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:00:46.858+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:00:46.922+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:00:46.922+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:00:47.032+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.352 seconds
[2024-11-20T12:01:19.127+0700] {processor.py:186} INFO - Started process (PID=171007) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:01:19.145+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:01:19.154+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:01:19.153+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:01:20.332+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:01:20.463+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:01:20.462+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:01:20.603+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:01:20.603+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:01:20.694+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.622 seconds
[2024-11-20T12:01:52.671+0700] {processor.py:186} INFO - Started process (PID=171441) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:01:52.674+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:01:52.683+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:01:52.681+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:01:53.078+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:01:53.180+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:01:53.179+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:01:53.258+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:01:53.257+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:01:53.323+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.672 seconds
[2024-11-20T12:02:24.060+0700] {processor.py:186} INFO - Started process (PID=171748) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:02:24.061+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:02:24.064+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:02:24.064+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:02:24.251+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:02:24.296+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:02:24.296+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:02:24.330+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:02:24.330+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:02:24.358+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.306 seconds
[2024-11-20T12:02:55.093+0700] {processor.py:186} INFO - Started process (PID=172042) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:02:55.097+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:02:55.110+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:02:55.107+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:02:55.694+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:02:55.851+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:02:55.850+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:02:55.977+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:02:55.976+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:02:56.121+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.055 seconds
[2024-11-20T12:03:29.023+0700] {processor.py:186} INFO - Started process (PID=172393) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:03:29.030+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:03:29.043+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:03:29.041+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:03:29.726+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:03:29.868+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:03:29.867+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:03:29.979+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:03:29.979+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:03:30.056+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.065 seconds
[2024-11-20T12:04:00.664+0700] {processor.py:186} INFO - Started process (PID=172818) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:04:00.667+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:04:00.677+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:04:00.674+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:04:01.483+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:04:01.588+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:04:01.587+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:04:01.743+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:04:01.742+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:04:01.803+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.163 seconds
[2024-11-20T12:04:32.282+0700] {processor.py:186} INFO - Started process (PID=173130) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:04:32.285+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:04:32.293+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:04:32.291+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:04:32.828+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:04:33.267+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:04:33.265+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:04:33.548+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:04:33.546+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:04:33.688+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.436 seconds
[2024-11-20T12:05:17.861+0700] {processor.py:186} INFO - Started process (PID=173489) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:05:17.873+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:05:17.885+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:05:17.882+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:05:19.277+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:05:19.751+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:05:19.748+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:05:19.911+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:05:19.909+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:05:20.506+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 2.709 seconds
[2024-11-20T12:05:50.947+0700] {processor.py:186} INFO - Started process (PID=173810) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:05:50.971+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:05:50.979+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:05:50.977+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:05:51.456+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:05:51.587+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:05:51.586+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:05:51.698+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:05:51.698+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:05:51.776+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.853 seconds
[2024-11-20T12:06:22.119+0700] {processor.py:186} INFO - Started process (PID=174453) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:06:22.145+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:06:22.157+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:06:22.154+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:06:23.805+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:06:24.277+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:06:24.276+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:06:24.477+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:06:24.477+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:06:24.583+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 2.522 seconds
[2024-11-20T12:06:55.301+0700] {processor.py:186} INFO - Started process (PID=174858) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:06:55.613+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:06:55.619+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:06:55.618+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:06:55.880+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:06:55.935+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:06:55.934+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:06:55.974+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:06:55.974+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:06:56.649+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.360 seconds
[2024-11-20T12:07:26.998+0700] {processor.py:186} INFO - Started process (PID=175158) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:07:27.000+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:07:27.005+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:07:27.004+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:07:27.300+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:07:27.378+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:07:27.378+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:07:27.435+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:07:27.434+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:07:27.486+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.499 seconds
[2024-11-20T12:07:59.207+0700] {processor.py:186} INFO - Started process (PID=175475) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:07:59.214+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:07:59.237+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:07:59.229+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:07:59.889+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:08:00.005+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:08:00.005+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:08:00.108+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:08:00.107+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:08:00.206+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.037 seconds
[2024-11-20T12:08:31.542+0700] {processor.py:186} INFO - Started process (PID=176007) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:08:31.547+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:08:31.558+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:08:31.556+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:08:32.252+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:08:32.387+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:08:32.385+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:08:32.510+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:08:32.509+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:08:32.607+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.103 seconds
[2024-11-20T12:09:04.475+0700] {processor.py:186} INFO - Started process (PID=176512) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:09:04.483+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:09:04.505+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:09:04.504+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:09:06.024+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:09:06.197+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:09:06.196+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:09:06.394+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:09:06.393+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:09:06.489+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 2.055 seconds
[2024-11-20T12:09:39.859+0700] {processor.py:186} INFO - Started process (PID=176796) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:09:39.869+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:09:39.890+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:09:39.889+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:09:41.218+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:09:41.914+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:09:41.912+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:09:42.398+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:09:42.397+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:09:42.529+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 2.770 seconds
[2024-11-20T12:10:20.207+0700] {processor.py:186} INFO - Started process (PID=177467) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:10:20.239+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:10:20.277+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:10:20.273+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:10:25.194+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:10:26.710+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:10:26.709+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:10:27.830+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:10:27.829+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:10:28.681+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 8.525 seconds
[2024-11-20T12:11:21.860+0700] {processor.py:186} INFO - Started process (PID=180063) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:11:21.905+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:11:22.072+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:11:22.052+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:11:33.346+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:11:34.511+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:11:34.470+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:11:35.031+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:11:35.030+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:11:35.232+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 13.788 seconds
[2024-11-20T12:12:06.797+0700] {processor.py:186} INFO - Started process (PID=180953) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:12:06.799+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:12:06.803+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:12:06.803+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:12:07.087+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:12:07.149+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:12:07.148+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:12:07.205+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:12:07.204+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:12:07.252+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.471 seconds
[2024-11-20T12:12:46.846+0700] {processor.py:186} INFO - Started process (PID=181329) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:12:46.851+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:12:46.864+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:12:46.862+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:12:48.447+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:12:48.796+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:12:48.792+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:12:48.946+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:12:48.937+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:12:49.076+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 2.262 seconds
[2024-11-20T12:13:19.189+0700] {processor.py:186} INFO - Started process (PID=181697) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:13:19.192+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:13:19.201+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:13:19.200+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:13:19.714+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:13:19.824+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:13:19.820+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:13:19.949+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:13:19.948+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:13:20.033+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.877 seconds
[2024-11-20T12:13:52.205+0700] {processor.py:186} INFO - Started process (PID=182041) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:13:52.233+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:13:52.265+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:13:52.263+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:13:55.375+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:13:55.650+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:13:55.648+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:13:55.842+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:13:55.842+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:13:56.218+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 4.254 seconds
[2024-11-20T12:14:40.496+0700] {processor.py:186} INFO - Started process (PID=182574) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:14:40.502+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:14:40.511+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:14:40.509+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:14:43.261+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:14:43.998+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:14:43.997+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:14:44.411+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:14:44.409+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:14:44.598+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 4.156 seconds
[2024-11-20T12:30:10.907+0700] {processor.py:186} INFO - Started process (PID=6548) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:30:10.910+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:30:10.915+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:30:10.914+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:30:11.211+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:30:11.281+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:30:11.281+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:30:11.328+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:30:11.327+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:30:11.360+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.461 seconds
[2024-11-20T12:30:41.470+0700] {processor.py:186} INFO - Started process (PID=6841) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:30:41.476+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:30:41.489+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:30:41.486+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:30:42.760+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:30:42.838+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:30:42.837+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:30:42.915+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:30:42.915+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:30:42.965+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.536 seconds
[2024-11-20T12:31:13.377+0700] {processor.py:186} INFO - Started process (PID=8003) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:31:13.385+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:31:13.414+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:31:13.391+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:31:14.397+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:31:14.588+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:31:14.587+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:31:14.698+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:31:14.698+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:31:14.782+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.454 seconds
[2024-11-20T12:31:44.892+0700] {processor.py:186} INFO - Started process (PID=8541) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:31:44.906+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:31:44.909+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:31:44.909+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:31:45.335+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:31:45.469+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:31:45.469+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:31:45.806+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:31:45.806+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:31:45.890+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.010 seconds
[2024-11-20T12:32:16.180+0700] {processor.py:186} INFO - Started process (PID=8752) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:32:16.182+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:32:16.186+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:32:16.185+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:32:16.679+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:32:16.740+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:32:16.739+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:32:16.783+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:32:16.782+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:32:16.838+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.674 seconds
[2024-11-20T12:32:47.461+0700] {processor.py:186} INFO - Started process (PID=8880) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:32:47.463+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:32:47.471+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:32:47.470+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:32:48.803+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:32:48.990+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:32:48.990+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:32:49.106+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:32:49.105+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:32:49.158+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.716 seconds
[2024-11-20T12:33:19.250+0700] {processor.py:186} INFO - Started process (PID=9013) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:33:19.252+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:33:19.255+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:33:19.254+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:33:19.840+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:33:19.911+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:33:19.911+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:33:19.957+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:33:19.956+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:33:20.010+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.772 seconds
[2024-11-20T12:33:51.061+0700] {processor.py:186} INFO - Started process (PID=9156) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:33:51.062+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:33:51.065+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:33:51.065+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:33:51.269+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:33:51.292+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:33:51.292+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:33:51.308+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:33:51.308+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:33:51.322+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.268 seconds
[2024-11-20T12:34:22.070+0700] {processor.py:186} INFO - Started process (PID=9375) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:34:22.073+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:34:22.076+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:34:22.075+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:34:22.179+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:34:22.315+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:34:22.314+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:34:22.335+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:34:22.335+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:34:22.351+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.289 seconds
[2024-11-20T12:34:52.425+0700] {processor.py:186} INFO - Started process (PID=9533) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:34:52.425+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:34:52.427+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:34:52.427+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:34:52.619+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:34:52.641+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:34:52.641+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:34:52.656+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:34:52.656+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:34:52.669+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.248 seconds
[2024-11-20T12:35:22.858+0700] {processor.py:186} INFO - Started process (PID=9723) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:35:22.862+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:35:22.865+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:35:22.864+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:35:23.091+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:35:23.117+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:35:23.117+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:35:23.132+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:35:23.132+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:35:23.144+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.293 seconds
[2024-11-20T12:35:53.174+0700] {processor.py:186} INFO - Started process (PID=9878) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:35:53.176+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:35:53.179+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:35:53.178+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:35:53.517+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:35:53.559+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:35:53.559+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:35:53.590+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:35:53.589+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:35:53.614+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.450 seconds
[2024-11-20T12:36:24.026+0700] {processor.py:186} INFO - Started process (PID=10189) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:36:24.029+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:36:24.033+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:36:24.032+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:36:24.278+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:36:24.304+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:36:24.303+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:36:24.324+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:36:24.324+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:36:24.343+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.325 seconds
[2024-11-20T12:36:54.421+0700] {processor.py:186} INFO - Started process (PID=10346) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:36:54.422+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:36:54.425+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:36:54.424+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:36:54.634+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:36:54.659+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:36:54.658+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:36:54.676+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:36:54.675+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:36:54.690+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.279 seconds
[2024-11-20T12:37:25.023+0700] {processor.py:186} INFO - Started process (PID=10480) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:37:25.024+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:37:25.027+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:37:25.026+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:37:25.262+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:37:25.286+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:37:25.286+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:37:25.303+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:37:25.303+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:37:25.318+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.308 seconds
[2024-11-20T12:37:55.364+0700] {processor.py:186} INFO - Started process (PID=10633) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:37:55.366+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:37:55.368+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:37:55.368+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:37:55.774+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:37:55.832+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:37:55.831+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:37:55.882+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:37:55.881+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:37:55.925+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.571 seconds
[2024-11-20T12:38:26.178+0700] {processor.py:186} INFO - Started process (PID=10765) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:38:26.180+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:38:26.182+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:38:26.182+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:38:26.356+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:38:26.400+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:38:26.400+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:38:26.441+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:38:26.441+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:38:26.471+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.300 seconds
[2024-11-20T12:38:56.790+0700] {processor.py:186} INFO - Started process (PID=11047) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:38:56.794+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:38:56.798+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:38:56.798+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:38:57.215+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:38:57.339+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:38:57.337+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:38:57.397+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:38:57.397+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:38:57.432+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.660 seconds
[2024-11-20T12:39:28.121+0700] {processor.py:186} INFO - Started process (PID=11402) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:39:28.123+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:39:28.126+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:39:28.126+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:39:28.299+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:39:28.339+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:39:28.339+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:39:28.370+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:39:28.370+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:39:28.399+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.287 seconds
[2024-11-20T12:39:58.967+0700] {processor.py:186} INFO - Started process (PID=12338) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:39:58.970+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:39:58.978+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:39:58.977+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:39:59.760+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:39:59.866+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:39:59.865+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:39:59.944+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:39:59.943+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:39:59.990+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.067 seconds
[2024-11-20T12:40:30.048+0700] {processor.py:186} INFO - Started process (PID=12590) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:40:30.051+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:40:30.053+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:40:30.053+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:40:30.203+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:40:30.293+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:40:30.293+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:40:30.334+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:40:30.334+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:40:30.358+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.318 seconds
[2024-11-20T12:41:00.588+0700] {processor.py:186} INFO - Started process (PID=12721) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:41:00.588+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:41:00.590+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:41:00.590+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:41:00.696+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:41:00.724+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:41:00.723+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:41:00.743+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:41:00.743+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:41:00.764+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.183 seconds
[2024-11-20T12:41:13.859+0700] {processor.py:186} INFO - Started process (PID=12802) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:41:13.861+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:41:13.863+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:41:13.863+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:41:14.021+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:41:14.066+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:41:14.066+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:41:14.129+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:41:14.129+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:41:14.182+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.333 seconds
[2024-11-20T12:41:44.398+0700] {processor.py:186} INFO - Started process (PID=13078) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:41:44.399+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:41:44.400+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:41:44.400+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:41:44.506+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:41:44.534+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:41:44.533+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:41:44.553+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:41:44.553+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:41:44.572+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.179 seconds
[2024-11-20T12:42:15.092+0700] {processor.py:186} INFO - Started process (PID=13307) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:42:15.095+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:42:15.097+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:42:15.096+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:42:15.209+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:42:15.236+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:42:15.236+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:42:15.265+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:42:15.265+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:42:15.282+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.196 seconds
[2024-11-20T12:42:45.699+0700] {processor.py:186} INFO - Started process (PID=13936) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:42:45.703+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:42:45.712+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:42:45.711+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:42:46.391+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:42:46.502+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:42:46.502+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:42:46.560+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:42:46.560+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:42:46.630+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.948 seconds
[2024-11-20T12:43:17.192+0700] {processor.py:186} INFO - Started process (PID=14593) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:43:17.193+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:43:17.205+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:43:17.203+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:43:17.513+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:43:17.599+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:43:17.598+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:43:17.667+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:43:17.666+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:43:17.712+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.534 seconds
[2024-11-20T12:43:48.591+0700] {processor.py:186} INFO - Started process (PID=14802) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:43:48.593+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:43:48.600+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:43:48.599+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:43:49.429+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:43:49.684+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:43:49.684+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:43:49.850+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:43:49.850+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:43:49.929+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.368 seconds
[2024-11-20T12:44:20.834+0700] {processor.py:186} INFO - Started process (PID=15275) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:44:20.837+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:44:20.841+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:44:20.841+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:44:21.217+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:44:21.271+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:44:21.271+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:44:21.312+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:44:21.312+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:44:21.344+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.531 seconds
[2024-11-20T12:44:51.584+0700] {processor.py:186} INFO - Started process (PID=15796) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:44:51.585+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:44:51.587+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:44:51.587+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:44:51.697+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:44:51.729+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:44:51.729+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:44:51.751+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:44:51.751+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:44:51.770+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.194 seconds
[2024-11-20T12:45:22.350+0700] {processor.py:186} INFO - Started process (PID=15966) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:45:22.352+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:45:22.357+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:45:22.357+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:45:23.034+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:45:23.166+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:45:23.164+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:45:23.237+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:45:23.237+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:45:23.327+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.992 seconds
[2024-11-20T12:45:53.863+0700] {processor.py:186} INFO - Started process (PID=16119) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:45:53.866+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:45:53.872+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:45:53.870+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:45:54.122+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:45:54.175+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:45:54.174+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:45:54.222+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:45:54.222+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:45:54.255+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.403 seconds
[2024-11-20T12:46:24.818+0700] {processor.py:186} INFO - Started process (PID=16455) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:46:24.820+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:46:24.823+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:46:24.823+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:46:25.152+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:46:25.257+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:46:25.256+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:46:25.342+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:46:25.342+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:46:25.407+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.601 seconds
[2024-11-20T12:46:55.862+0700] {processor.py:186} INFO - Started process (PID=16596) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:46:55.868+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:46:55.875+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:46:55.874+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:46:56.389+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:46:56.480+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:46:56.480+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:46:56.677+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:46:56.677+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:46:56.808+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.000 seconds
[2024-11-20T12:47:27.080+0700] {processor.py:186} INFO - Started process (PID=16757) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:47:27.081+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:47:27.084+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:47:27.083+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:47:27.264+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:47:27.354+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:47:27.353+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:47:27.425+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:47:27.424+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:47:27.479+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.406 seconds
[2024-11-20T12:47:57.991+0700] {processor.py:186} INFO - Started process (PID=16948) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:47:58.000+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:47:58.007+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:47:58.006+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:47:58.458+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:47:58.577+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:47:58.576+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:47:58.689+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:47:58.689+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:47:58.757+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.803 seconds
[2024-11-20T12:48:29.272+0700] {processor.py:186} INFO - Started process (PID=17187) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:48:29.275+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:48:29.281+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:48:29.280+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:48:29.686+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:48:29.864+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:48:29.863+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:48:29.990+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:48:29.989+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:48:30.065+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.812 seconds
[2024-11-20T12:49:01.301+0700] {processor.py:186} INFO - Started process (PID=17342) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:49:01.307+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:49:01.318+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:49:01.317+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:49:02.215+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:49:02.429+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:49:02.428+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:49:02.637+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:49:02.630+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:49:02.930+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.651 seconds
[2024-11-20T12:49:33.097+0700] {processor.py:186} INFO - Started process (PID=17494) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:49:33.100+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:49:33.105+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:49:33.104+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:49:33.353+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:49:33.413+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:49:33.413+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:49:33.505+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:49:33.504+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:49:33.564+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.477 seconds
[2024-11-20T12:50:03.844+0700] {processor.py:186} INFO - Started process (PID=17704) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:50:03.847+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:50:03.850+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:50:03.850+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:50:04.009+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:50:04.048+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:50:04.047+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:50:04.079+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:50:04.079+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:50:04.104+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.268 seconds
[2024-11-20T12:50:34.543+0700] {processor.py:186} INFO - Started process (PID=17949) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:50:34.547+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:50:34.553+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:50:34.552+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:50:35.022+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:50:35.210+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:50:35.210+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:50:35.342+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:50:35.337+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:50:35.423+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.897 seconds
[2024-11-20T12:51:05.641+0700] {processor.py:186} INFO - Started process (PID=18295) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:51:05.674+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:51:05.678+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:51:05.677+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:51:06.557+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:51:06.792+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:51:06.790+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:51:07.118+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:51:07.118+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:51:07.332+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.720 seconds
[2024-11-20T12:51:50.982+0700] {processor.py:186} INFO - Started process (PID=18565) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:51:50.993+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:51:51.004+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:51:51.004+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:51:51.577+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:51:51.640+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:51:51.639+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:51:51.759+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:51:51.758+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:51:51.830+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.869 seconds
[2024-11-20T12:52:22.461+0700] {processor.py:186} INFO - Started process (PID=18790) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:52:22.466+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:52:22.474+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:52:22.473+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:52:24.142+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:52:24.302+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:52:24.301+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:52:24.397+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:52:24.396+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:52:24.496+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 2.080 seconds
[2024-11-20T12:52:54.708+0700] {processor.py:186} INFO - Started process (PID=18931) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:52:54.712+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:52:54.729+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:52:54.723+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:52:55.729+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:52:55.912+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:52:55.911+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:52:56.005+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:52:56.005+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:52:56.107+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.431 seconds
[2024-11-20T12:53:26.386+0700] {processor.py:186} INFO - Started process (PID=19172) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:53:26.391+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:53:26.400+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:53:26.399+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:53:26.730+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:53:26.828+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:53:26.828+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:53:26.885+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:53:26.884+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:53:26.942+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.580 seconds
[2024-11-20T12:53:57.652+0700] {processor.py:186} INFO - Started process (PID=19306) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:53:57.656+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:53:57.665+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:53:57.664+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:53:57.948+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:53:58.022+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:53:58.022+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:53:58.092+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:53:58.092+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:53:58.137+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.496 seconds
[2024-11-20T12:54:28.354+0700] {processor.py:186} INFO - Started process (PID=19450) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:54:28.362+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:54:28.370+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:54:28.369+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:54:28.861+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:54:28.943+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:54:28.942+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:54:29.018+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:54:29.017+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:54:29.066+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.762 seconds
[2024-11-20T12:54:59.459+0700] {processor.py:186} INFO - Started process (PID=19604) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:54:59.467+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:54:59.478+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:54:59.475+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:55:00.223+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:55:00.396+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:55:00.395+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:55:00.519+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:55:00.518+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:55:00.610+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.192 seconds
[2024-11-20T12:55:30.817+0700] {processor.py:186} INFO - Started process (PID=19797) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:55:30.819+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:55:30.822+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:55:30.821+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:55:31.022+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:55:31.072+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:55:31.072+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:55:31.114+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:55:31.113+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:55:31.142+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.335 seconds
[2024-11-20T12:56:02.226+0700] {processor.py:186} INFO - Started process (PID=20036) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:56:02.229+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:56:02.238+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:56:02.237+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:56:03.000+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:56:03.119+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:56:03.118+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:56:03.227+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:56:03.226+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:56:03.312+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.114 seconds
[2024-11-20T12:56:33.980+0700] {processor.py:186} INFO - Started process (PID=20193) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:56:33.999+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:56:34.018+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:56:34.015+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:56:36.197+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:56:36.709+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:56:36.708+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:56:37.108+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:56:37.106+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:56:37.436+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 3.547 seconds
[2024-11-20T12:57:08.134+0700] {processor.py:186} INFO - Started process (PID=20358) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:57:08.136+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:57:08.142+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:57:08.141+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:57:08.571+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:57:08.690+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:57:08.689+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:57:08.789+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:57:08.789+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:57:08.834+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.717 seconds
[2024-11-20T12:57:38.902+0700] {processor.py:186} INFO - Started process (PID=20486) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:57:38.906+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:57:38.910+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:57:38.909+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:57:39.242+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:57:39.331+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:57:39.331+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:57:39.389+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:57:39.389+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:57:39.425+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.539 seconds
[2024-11-20T12:58:09.624+0700] {processor.py:186} INFO - Started process (PID=20629) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:58:09.628+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:58:09.635+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:58:09.634+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:58:10.065+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:58:10.155+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:58:10.155+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:58:10.243+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:58:10.243+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:58:10.294+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.703 seconds
[2024-11-20T12:58:40.964+0700] {processor.py:186} INFO - Started process (PID=20756) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:58:40.967+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:58:40.971+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:58:40.969+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:58:41.106+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:58:41.138+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:58:41.137+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:58:41.167+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:58:41.167+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:58:41.191+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.237 seconds
[2024-11-20T12:59:11.921+0700] {processor.py:186} INFO - Started process (PID=20900) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:59:11.927+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:59:11.938+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:59:11.937+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:59:12.374+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:59:12.464+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:59:12.463+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:59:12.536+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:59:12.535+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:59:12.603+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.717 seconds
[2024-11-20T12:59:43.120+0700] {processor.py:186} INFO - Started process (PID=21027) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:59:43.133+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T12:59:43.141+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:59:43.140+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:59:43.738+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T12:59:43.868+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:59:43.867+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T12:59:43.996+0700] {logging_mixin.py:190} INFO - [2024-11-20T12:59:43.995+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 05:00:00+00:00, run_after=2024-11-20 06:00:00+00:00
[2024-11-20T12:59:44.099+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.015 seconds
[2024-11-20T13:00:21.796+0700] {processor.py:186} INFO - Started process (PID=21214) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:00:21.812+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:00:21.839+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:00:21.836+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:00:25.541+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:00:26.017+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:00:26.016+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:00:26.938+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:00:26.929+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:00:27.186+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 5.537 seconds
[2024-11-20T13:01:04.951+0700] {processor.py:186} INFO - Started process (PID=21485) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:01:05.041+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:01:05.068+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:01:05.067+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:01:05.672+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:01:05.769+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:01:05.768+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:01:05.855+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:01:05.855+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:01:06.493+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.593 seconds
[2024-11-20T13:02:34.512+0700] {processor.py:186} INFO - Started process (PID=22115) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:02:34.576+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:02:34.646+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:02:34.640+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:02:51.679+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:02:53.055+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:02:53.018+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:02:54.442+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:02:54.419+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:02:55.038+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 20.873 seconds
[2024-11-20T13:04:25.964+0700] {processor.py:186} INFO - Started process (PID=22372) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:04:25.973+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:04:25.994+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:04:25.993+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:04:28.879+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:04:29.452+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:04:29.436+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:04:30.090+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:04:30.089+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:04:30.379+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 4.563 seconds
[2024-11-20T13:05:06.164+0700] {processor.py:186} INFO - Started process (PID=22559) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:05:06.179+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:05:06.204+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:05:06.202+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:05:07.549+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:05:07.748+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:05:07.747+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:05:07.877+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:05:07.876+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:05:07.948+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.858 seconds
[2024-11-20T13:05:41.846+0700] {processor.py:186} INFO - Started process (PID=22793) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:05:41.859+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:05:41.892+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:05:41.886+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:05:42.995+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:05:43.227+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:05:43.226+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:05:43.410+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:05:43.408+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:05:43.509+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.746 seconds
[2024-11-20T13:06:15.158+0700] {processor.py:186} INFO - Started process (PID=22941) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:06:15.163+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:06:15.177+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:06:15.175+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:06:16.831+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:06:17.232+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:06:17.228+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:06:17.454+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:06:17.452+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:06:17.572+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 2.460 seconds
[2024-11-20T13:06:49.889+0700] {processor.py:186} INFO - Started process (PID=23107) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:06:49.906+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:06:49.979+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:06:49.978+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:06:51.970+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:06:52.210+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:06:52.209+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:06:52.407+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:06:52.406+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:06:52.556+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 2.814 seconds
[2024-11-20T13:07:23.211+0700] {processor.py:186} INFO - Started process (PID=23272) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:07:23.217+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:07:23.224+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:07:23.224+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:07:23.602+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:07:23.697+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:07:23.696+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:07:23.781+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:07:23.780+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:07:23.848+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.651 seconds
[2024-11-20T13:07:54.435+0700] {processor.py:186} INFO - Started process (PID=23401) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:07:54.441+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:07:54.469+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:07:54.467+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:07:56.432+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:07:56.680+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:07:56.679+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:07:56.904+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:07:56.903+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:07:57.027+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 2.636 seconds
[2024-11-20T13:08:27.899+0700] {processor.py:186} INFO - Started process (PID=23553) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:08:27.904+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:08:27.912+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:08:27.911+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:08:28.393+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:08:28.521+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:08:28.520+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:08:28.630+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:08:28.629+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:08:28.707+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.827 seconds
[2024-11-20T13:08:59.595+0700] {processor.py:186} INFO - Started process (PID=23687) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:08:59.598+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:08:59.604+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:08:59.603+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:08:59.962+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:09:00.068+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:09:00.068+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:09:00.152+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:09:00.151+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:09:00.205+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.628 seconds
[2024-11-20T13:09:30.315+0700] {processor.py:186} INFO - Started process (PID=23824) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:09:30.317+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:09:30.324+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:09:30.323+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:09:30.726+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:09:30.826+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:09:30.825+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:09:30.939+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:09:30.938+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:09:31.024+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.730 seconds
[2024-11-20T13:10:02.043+0700] {processor.py:186} INFO - Started process (PID=24053) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:10:02.050+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:10:02.070+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:10:02.067+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:10:03.198+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:10:03.422+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:10:03.421+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:10:03.629+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:10:03.627+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:10:03.752+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.759 seconds
[2024-11-20T13:10:35.900+0700] {processor.py:186} INFO - Started process (PID=24799) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:10:35.912+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:10:35.951+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:10:35.946+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:10:37.738+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:10:38.063+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:10:38.059+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:10:38.279+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:10:38.278+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:10:38.376+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 2.591 seconds
[2024-11-20T13:11:14.929+0700] {processor.py:186} INFO - Started process (PID=24970) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:11:14.936+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:11:14.960+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:11:14.957+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:11:19.654+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:11:20.060+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:11:20.058+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:11:20.569+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:11:20.568+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:11:20.815+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 5.926 seconds
[2024-11-20T13:12:13.081+0700] {processor.py:186} INFO - Started process (PID=25159) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:12:13.106+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:12:13.130+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:12:13.127+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:12:14.922+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:12:15.170+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:12:15.169+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:12:15.589+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:12:15.578+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:12:16.505+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 3.507 seconds
[2024-11-20T13:12:46.832+0700] {processor.py:186} INFO - Started process (PID=25389) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:12:46.955+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:12:46.968+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:12:46.966+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:12:47.767+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:12:47.909+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:12:47.909+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:12:48.036+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:12:48.035+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:12:48.138+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.343 seconds
[2024-11-20T13:13:18.855+0700] {processor.py:186} INFO - Started process (PID=25531) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:13:18.858+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:13:18.860+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:13:18.860+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:13:19.048+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:13:19.095+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:13:19.094+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:13:19.137+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:13:19.137+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:13:19.169+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.322 seconds
[2024-11-20T13:13:49.660+0700] {processor.py:186} INFO - Started process (PID=25690) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:13:49.665+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:13:49.676+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:13:49.675+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:13:50.320+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:13:50.479+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:13:50.478+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:13:50.610+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:13:50.610+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:13:50.705+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.073 seconds
[2024-11-20T13:14:20.814+0700] {processor.py:186} INFO - Started process (PID=25833) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:14:20.817+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:14:20.821+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:14:20.820+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:14:21.242+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:14:21.335+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:14:21.334+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:14:21.396+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:14:21.395+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:14:21.451+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.650 seconds
[2024-11-20T13:14:51.773+0700] {processor.py:186} INFO - Started process (PID=25963) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:14:51.779+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:14:51.790+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:14:51.788+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:14:52.756+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:14:52.933+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:14:52.932+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:14:53.077+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:14:53.076+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:14:53.318+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.583 seconds
[2024-11-20T13:15:23.740+0700] {processor.py:186} INFO - Started process (PID=26106) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:15:23.742+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:15:23.748+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:15:23.747+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:15:24.148+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:15:24.252+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:15:24.251+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:15:24.340+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:15:24.339+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:15:24.395+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.675 seconds
[2024-11-20T13:15:54.460+0700] {processor.py:186} INFO - Started process (PID=26241) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:15:54.462+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:15:54.465+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:15:54.464+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:15:54.657+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:15:54.712+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:15:54.711+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:15:54.749+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:15:54.749+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:15:54.779+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.326 seconds
[2024-11-20T13:16:25.046+0700] {processor.py:186} INFO - Started process (PID=26376) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:16:25.048+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:16:25.054+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:16:25.053+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:16:25.432+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:16:25.541+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:16:25.541+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:16:25.607+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:16:25.606+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:16:25.660+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.628 seconds
[2024-11-20T13:16:56.277+0700] {processor.py:186} INFO - Started process (PID=26537) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:16:56.281+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:16:56.285+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:16:56.285+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:16:56.529+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:16:56.586+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:16:56.586+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:17:05.162+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:17:05.161+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:17:05.282+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 9.017 seconds
[2024-11-20T13:17:35.416+0700] {processor.py:186} INFO - Started process (PID=26711) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:17:35.420+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:17:35.425+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:17:35.424+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:17:35.812+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:17:35.926+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:17:35.926+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:17:36.388+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:17:36.388+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:17:36.441+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.040 seconds
[2024-11-20T13:18:07.296+0700] {processor.py:186} INFO - Started process (PID=26912) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:18:07.300+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:18:07.304+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:18:07.303+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:18:07.886+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:18:07.958+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:18:07.957+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:18:08.009+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:18:08.008+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:18:08.064+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.778 seconds
[2024-11-20T13:18:38.876+0700] {processor.py:186} INFO - Started process (PID=27052) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:18:38.886+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:18:38.901+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:18:38.900+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:18:39.938+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:18:40.081+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:18:40.079+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:18:40.251+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:18:40.249+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:18:40.351+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.534 seconds
[2024-11-20T13:19:10.528+0700] {processor.py:186} INFO - Started process (PID=27191) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:19:10.530+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:19:10.534+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:19:10.534+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:19:11.144+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:19:11.215+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:19:11.215+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:19:11.266+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:19:11.265+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:19:11.309+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.792 seconds
[2024-11-20T13:19:41.503+0700] {processor.py:186} INFO - Started process (PID=27324) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:19:41.507+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:19:41.514+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:19:41.513+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:19:41.977+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:19:42.081+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:19:42.080+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:19:42.720+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:19:42.719+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:19:42.790+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.308 seconds
[2024-11-20T13:20:12.948+0700] {processor.py:186} INFO - Started process (PID=27461) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:20:12.950+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:20:12.954+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:20:12.954+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:20:13.491+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:20:13.568+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:20:13.568+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:20:13.615+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:20:13.615+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:20:13.655+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.716 seconds
[2024-11-20T13:20:43.819+0700] {processor.py:186} INFO - Started process (PID=27593) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:20:43.821+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:20:43.826+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:20:43.826+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:20:44.296+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:20:44.347+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:20:44.347+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:20:44.388+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:20:44.388+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:20:44.420+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.614 seconds
[2024-11-20T13:21:15.276+0700] {processor.py:186} INFO - Started process (PID=27725) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:21:15.280+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:21:15.284+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:21:15.284+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:21:15.798+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:21:15.864+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:21:15.864+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:21:15.918+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:21:15.918+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:21:15.955+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.694 seconds
[2024-11-20T13:21:46.178+0700] {processor.py:186} INFO - Started process (PID=27862) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:21:46.179+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:21:46.183+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:21:46.182+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:21:46.678+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:21:46.730+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:21:46.730+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:21:46.767+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:21:46.767+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:21:46.800+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.631 seconds
[2024-11-20T13:22:17.559+0700] {processor.py:186} INFO - Started process (PID=27995) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:22:17.560+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:22:17.563+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:22:17.563+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:22:17.872+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:22:17.919+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:22:17.919+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:22:17.950+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:22:17.950+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:22:17.979+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.429 seconds
[2024-11-20T13:22:48.034+0700] {processor.py:186} INFO - Started process (PID=28159) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:22:48.035+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:22:48.037+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:22:48.037+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:22:48.418+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:22:48.465+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:22:48.465+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:22:48.499+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:22:48.499+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:22:48.529+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.503 seconds
[2024-11-20T13:23:18.829+0700] {processor.py:186} INFO - Started process (PID=28305) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:23:18.831+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:23:18.834+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:23:18.833+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:23:19.233+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:23:19.286+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:23:19.286+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:23:19.324+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:23:19.324+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:23:19.357+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.535 seconds
[2024-11-20T13:23:49.874+0700] {processor.py:186} INFO - Started process (PID=28465) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:23:49.881+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:23:49.905+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:23:49.901+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:23:51.347+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:23:51.435+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:23:51.434+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:23:51.501+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:23:51.500+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:23:51.556+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.771 seconds
[2024-11-20T13:24:21.888+0700] {processor.py:186} INFO - Started process (PID=28616) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:24:21.892+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:24:21.902+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:24:21.900+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:24:23.304+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:24:23.517+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:24:23.515+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:24:23.702+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:24:23.695+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:24:23.909+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 2.056 seconds
[2024-11-20T13:24:54.312+0700] {processor.py:186} INFO - Started process (PID=28762) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:24:54.313+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:24:54.316+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:24:54.315+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:24:54.660+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:24:54.700+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:24:54.700+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:24:54.727+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:24:54.727+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:24:54.753+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.448 seconds
[2024-11-20T13:25:25.213+0700] {processor.py:186} INFO - Started process (PID=28917) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:25:25.214+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:25:25.217+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:25:25.217+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:25:25.609+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:25:25.655+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:25:25.655+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:25:25.687+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:25:25.687+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:25:25.714+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.511 seconds
[2024-11-20T13:25:58.069+0700] {processor.py:186} INFO - Started process (PID=29043) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:25:58.071+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:25:58.076+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:25:58.075+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:25:58.799+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:25:58.881+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:25:58.880+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:25:58.946+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:25:58.945+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:25:58.996+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.950 seconds
[2024-11-20T13:26:29.832+0700] {processor.py:186} INFO - Started process (PID=29178) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:26:29.833+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:26:29.836+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:26:29.836+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:26:29.971+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:26:30.011+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:26:30.011+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:26:30.038+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:26:30.038+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:26:30.063+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.241 seconds
[2024-11-20T13:27:00.338+0700] {processor.py:186} INFO - Started process (PID=29334) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:27:00.341+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:27:00.352+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:27:00.350+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:27:00.955+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:27:01.069+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:27:01.068+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:27:01.144+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:27:01.143+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:27:01.217+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.917 seconds
[2024-11-20T13:27:32.716+0700] {processor.py:186} INFO - Started process (PID=29626) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:27:32.720+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:27:32.730+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:27:32.727+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:27:34.395+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:27:34.626+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:27:34.610+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:27:34.995+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:27:34.994+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:27:35.196+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 2.545 seconds
[2024-11-20T13:28:05.470+0700] {processor.py:186} INFO - Started process (PID=29948) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:28:05.473+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:28:05.482+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:28:05.481+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:28:06.096+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:28:06.269+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:28:06.268+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:28:06.369+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:28:06.368+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:28:06.447+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.003 seconds
[2024-11-20T13:28:37.331+0700] {processor.py:186} INFO - Started process (PID=30084) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:28:37.344+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:28:37.379+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:28:37.373+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:28:39.281+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:28:39.818+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:28:39.816+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:28:40.042+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:28:40.037+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:28:40.252+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 3.049 seconds
[2024-11-20T13:29:10.887+0700] {processor.py:186} INFO - Started process (PID=30444) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:29:10.907+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:29:10.933+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:29:10.927+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:29:11.856+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:29:11.992+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:29:11.991+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:29:12.146+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:29:12.145+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:29:12.266+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.464 seconds
[2024-11-20T13:29:43.039+0700] {processor.py:186} INFO - Started process (PID=30618) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:29:43.049+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:29:43.065+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:29:43.060+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:29:43.862+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:29:43.993+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:29:43.992+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:29:44.113+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:29:44.112+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:29:44.207+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.220 seconds
[2024-11-20T13:30:18.958+0700] {processor.py:186} INFO - Started process (PID=30797) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:30:18.975+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:30:18.993+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:30:18.992+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:30:21.677+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:30:22.134+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:30:22.123+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:30:22.352+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:30:22.350+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:30:22.546+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 3.708 seconds
[2024-11-20T13:30:53.277+0700] {processor.py:186} INFO - Started process (PID=30941) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:30:53.292+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:30:53.307+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:30:53.306+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:30:54.413+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:30:54.635+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:30:54.623+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:30:54.803+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:30:54.802+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:30:54.953+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.724 seconds
[2024-11-20T13:31:25.647+0700] {processor.py:186} INFO - Started process (PID=31184) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:31:25.658+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:31:25.684+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:31:25.682+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:31:27.411+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:31:27.774+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:31:27.773+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:31:27.935+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:31:27.933+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:31:28.044+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 2.479 seconds
[2024-11-20T13:32:01.087+0700] {processor.py:186} INFO - Started process (PID=31374) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:32:01.108+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:32:01.157+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:32:01.154+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:32:02.923+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:32:03.259+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:32:03.255+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:32:03.518+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:32:03.517+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:32:03.705+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 2.755 seconds
[2024-11-20T13:32:39.858+0700] {processor.py:186} INFO - Started process (PID=31810) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:32:39.892+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:32:39.926+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:32:39.924+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:32:41.781+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:32:41.978+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:32:41.977+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:32:42.126+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:32:42.125+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:32:42.549+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 2.864 seconds
[2024-11-20T13:33:27.523+0700] {processor.py:186} INFO - Started process (PID=32344) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:33:27.529+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:33:27.540+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:33:27.538+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:33:28.110+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:33:28.226+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:33:28.225+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:33:28.317+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:33:28.316+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:33:28.409+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.919 seconds
[2024-11-20T13:34:00.320+0700] {processor.py:186} INFO - Started process (PID=32531) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:34:00.328+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:34:00.340+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:34:00.339+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:34:01.203+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:34:01.343+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:34:01.343+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:34:01.462+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:34:01.461+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:34:01.551+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.267 seconds
[2024-11-20T13:34:43.549+0700] {processor.py:186} INFO - Started process (PID=32710) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:34:43.617+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:34:43.725+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:34:43.714+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:34:50.673+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:34:52.462+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:34:52.456+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:34:53.184+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:34:53.182+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:34:53.897+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 10.411 seconds
[2024-11-20T13:35:58.003+0700] {processor.py:186} INFO - Started process (PID=33129) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:35:58.008+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:35:58.038+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:35:58.036+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:36:00.403+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:36:00.688+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:36:00.687+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:36:00.929+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:36:00.928+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:36:01.200+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 3.311 seconds
[2024-11-20T13:36:34.749+0700] {processor.py:186} INFO - Started process (PID=33739) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:36:34.763+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:36:34.774+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:36:34.772+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:36:38.223+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:36:39.176+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:36:39.174+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:36:39.472+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:36:39.469+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:36:39.923+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 5.270 seconds
[2024-11-20T13:37:25.669+0700] {processor.py:186} INFO - Started process (PID=34049) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:37:25.706+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:37:25.720+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:37:25.718+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:37:26.339+0700] {processor.py:925} INFO - DAG(s) 'scrape_twitter' retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:37:26.489+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:37:26.488+0700] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-20T13:37:26.598+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:37:26.597+0700] {dag.py:4180} INFO - Setting next_dagrun for scrape_twitter to 2024-11-20 06:00:00+00:00, run_after=2024-11-20 07:00:00+00:00
[2024-11-20T13:37:26.734+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.122 seconds
[2024-11-20T13:38:00.057+0700] {processor.py:186} INFO - Started process (PID=34306) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:38:00.065+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:38:00.122+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:38:00.120+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:38:00.287+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:38:00.281+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:38:00.326+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:38:00.914+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.921 seconds
[2024-11-20T13:38:53.643+0700] {processor.py:186} INFO - Started process (PID=34489) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:38:53.651+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:38:53.664+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:38:53.662+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:38:53.739+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:38:53.728+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:38:53.743+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:38:53.984+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.414 seconds
[2024-11-20T13:39:24.435+0700] {processor.py:186} INFO - Started process (PID=34702) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:39:24.437+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:39:24.443+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:39:24.442+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:39:24.479+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:39:24.475+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:39:24.482+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:39:24.585+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.166 seconds
[2024-11-20T13:39:54.852+0700] {processor.py:186} INFO - Started process (PID=34914) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:39:54.861+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:39:54.877+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:39:54.876+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:39:54.967+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:39:54.955+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:39:54.969+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:39:55.194+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.416 seconds
[2024-11-20T13:40:25.412+0700] {processor.py:186} INFO - Started process (PID=35142) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:40:25.413+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:40:25.415+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:40:25.414+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:40:25.427+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:40:25.424+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:40:25.427+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:40:25.472+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.068 seconds
[2024-11-20T13:40:55.908+0700] {processor.py:186} INFO - Started process (PID=35399) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:40:55.910+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:40:55.913+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:40:55.912+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:40:55.937+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:40:55.931+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:40:55.939+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:40:56.054+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.161 seconds
[2024-11-20T13:41:26.706+0700] {processor.py:186} INFO - Started process (PID=35595) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:41:26.708+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:41:26.709+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:41:26.709+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:41:26.720+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:41:26.719+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:41:26.721+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:41:26.767+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.070 seconds
[2024-11-20T13:41:57.100+0700] {processor.py:186} INFO - Started process (PID=35740) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:41:57.101+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:41:57.103+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:41:57.102+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:41:57.117+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:41:57.115+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:41:57.118+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:41:57.171+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.080 seconds
[2024-11-20T13:42:28.130+0700] {processor.py:186} INFO - Started process (PID=35901) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:42:28.133+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:42:28.135+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:42:28.134+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:42:28.149+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:42:28.147+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:42:28.150+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:42:28.201+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.080 seconds
[2024-11-20T13:42:58.569+0700] {processor.py:186} INFO - Started process (PID=36158) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:42:58.569+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:42:58.571+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:42:58.570+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:42:58.580+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:42:58.579+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:42:58.580+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:42:58.614+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.053 seconds
[2024-11-20T13:43:28.740+0700] {processor.py:186} INFO - Started process (PID=36481) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:43:28.742+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:43:28.747+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:43:28.745+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:43:28.774+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:43:28.770+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:43:28.775+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:43:28.879+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.152 seconds
[2024-11-20T13:44:00.559+0700] {processor.py:186} INFO - Started process (PID=36715) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:44:00.564+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:44:00.571+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:44:00.569+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:44:00.617+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:44:00.607+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:44:00.620+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:44:00.872+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.341 seconds
[2024-11-20T13:44:31.168+0700] {processor.py:186} INFO - Started process (PID=36927) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:44:31.173+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:44:31.180+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:44:31.179+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:44:31.238+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:44:31.233+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:44:31.240+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:44:31.399+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.269 seconds
[2024-11-20T13:45:01.634+0700] {processor.py:186} INFO - Started process (PID=37159) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:45:01.636+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:45:01.638+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:45:01.638+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:45:01.654+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:45:01.652+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:45:01.655+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:45:01.712+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.089 seconds
[2024-11-20T13:45:32.156+0700] {processor.py:186} INFO - Started process (PID=37552) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:45:32.161+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:45:32.169+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:45:32.167+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:45:32.238+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:45:32.230+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:45:32.258+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:45:32.491+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.365 seconds
[2024-11-20T13:46:12.548+0700] {processor.py:186} INFO - Started process (PID=38024) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:46:12.551+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:46:12.554+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:46:12.553+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:46:12.579+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:46:12.576+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:46:12.581+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:46:12.723+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.223 seconds
[2024-11-20T13:46:48.114+0700] {processor.py:186} INFO - Started process (PID=38366) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:46:48.150+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:46:48.175+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:46:48.170+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:46:48.335+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:46:48.323+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:46:48.347+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:46:48.584+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.757 seconds
[2024-11-20T13:47:35.292+0700] {processor.py:186} INFO - Started process (PID=39008) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:47:35.296+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:47:35.299+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:47:35.298+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:47:35.326+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:47:35.322+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:47:35.328+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:47:35.428+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.162 seconds
[2024-11-20T13:48:06.940+0700] {processor.py:186} INFO - Started process (PID=39342) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:48:06.950+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:48:06.970+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:48:06.963+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:48:07.060+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:48:07.028+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:48:07.074+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:48:07.419+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.582 seconds
[2024-11-20T13:48:51.956+0700] {processor.py:186} INFO - Started process (PID=39652) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:48:51.963+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:48:51.973+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:48:51.969+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:48:52.120+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:48:52.089+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:48:52.127+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:48:52.451+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.555 seconds
[2024-11-20T13:49:22.784+0700] {processor.py:186} INFO - Started process (PID=39957) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:49:22.786+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:49:22.789+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:49:22.789+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:49:22.833+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:49:22.827+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:49:22.834+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:49:22.979+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.224 seconds
[2024-11-20T13:50:32.319+0700] {processor.py:186} INFO - Started process (PID=40440) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:50:32.357+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:50:32.391+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:50:32.375+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:50:32.508+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:50:32.495+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:50:32.549+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:50:32.994+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.104 seconds
[2024-11-20T13:52:12.528+0700] {processor.py:186} INFO - Started process (PID=41031) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:52:12.539+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:52:12.570+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:52:12.565+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:52:12.813+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:52:12.776+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:52:12.824+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:52:13.322+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.960 seconds
[2024-11-20T13:52:50.779+0700] {processor.py:186} INFO - Started process (PID=41382) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:52:50.781+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:52:50.787+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:52:50.783+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:52:50.820+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:52:50.816+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:52:50.822+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:52:50.999+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.257 seconds
[2024-11-20T13:53:22.144+0700] {processor.py:186} INFO - Started process (PID=41642) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:53:22.146+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:53:22.157+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:53:22.156+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:53:22.207+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:53:22.204+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:53:22.212+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:53:22.415+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.324 seconds
[2024-11-20T13:53:52.519+0700] {processor.py:186} INFO - Started process (PID=41930) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:53:52.521+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:53:52.523+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:53:52.522+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:53:52.540+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:53:52.538+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:53:52.541+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:53:52.652+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.144 seconds
[2024-11-20T13:54:23.031+0700] {processor.py:186} INFO - Started process (PID=42195) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:54:23.036+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:54:23.042+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:54:23.042+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:54:23.076+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:54:23.073+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:54:23.077+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:54:23.154+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.146 seconds
[2024-11-20T13:54:53.277+0700] {processor.py:186} INFO - Started process (PID=42467) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:54:53.290+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:54:53.305+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:54:53.302+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:54:53.359+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:54:53.349+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:54:53.361+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:54:53.571+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.322 seconds
[2024-11-20T13:55:23.805+0700] {processor.py:186} INFO - Started process (PID=42740) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:55:23.806+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:55:23.807+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:55:23.807+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:55:23.820+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:55:23.818+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:55:23.821+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:55:23.903+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.108 seconds
[2024-11-20T13:55:54.231+0700] {processor.py:186} INFO - Started process (PID=43005) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:55:54.234+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:55:54.237+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:55:54.236+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:55:54.267+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:55:54.260+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:55:54.273+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:55:54.497+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.291 seconds
[2024-11-20T13:56:26.907+0700] {processor.py:186} INFO - Started process (PID=43274) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:56:26.914+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:56:26.921+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:56:26.920+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:56:26.973+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:56:26.969+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:56:26.976+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:56:27.195+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.374 seconds
[2024-11-20T13:56:57.877+0700] {processor.py:186} INFO - Started process (PID=43593) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:56:57.881+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:56:57.890+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:56:57.886+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:56:57.969+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:56:57.962+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:56:57.979+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:56:58.346+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.514 seconds
[2024-11-20T13:57:28.752+0700] {processor.py:186} INFO - Started process (PID=44145) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:57:28.754+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:57:28.760+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:57:28.756+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:57:28.783+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:57:28.781+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:57:28.784+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:57:28.908+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.174 seconds
[2024-11-20T13:57:59.501+0700] {processor.py:186} INFO - Started process (PID=44442) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:57:59.522+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:57:59.531+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:57:59.529+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:57:59.610+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:57:59.598+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:57:59.612+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:57:59.778+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.304 seconds
[2024-11-20T13:58:30.076+0700] {processor.py:186} INFO - Started process (PID=44706) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:58:30.079+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:58:30.080+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:58:30.080+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:58:30.095+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:58:30.093+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:58:30.095+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:58:30.138+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.069 seconds
[2024-11-20T13:59:00.849+0700] {processor.py:186} INFO - Started process (PID=44982) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:59:00.853+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:59:00.855+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:59:00.855+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:59:00.873+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:59:00.870+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:59:00.874+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:59:00.954+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.126 seconds
[2024-11-20T13:59:31.405+0700] {processor.py:186} INFO - Started process (PID=45250) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:59:31.415+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T13:59:31.436+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:59:31.435+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:59:31.588+0700] {logging_mixin.py:190} INFO - [2024-11-20T13:59:31.570+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T13:59:31.590+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T13:59:31.829+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.509 seconds
[2024-11-20T14:00:02.279+0700] {processor.py:186} INFO - Started process (PID=45508) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:00:02.280+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T14:00:02.295+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:00:02.290+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:00:02.394+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:00:02.385+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T14:00:02.400+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:00:02.598+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.346 seconds
[2024-11-20T14:00:33.451+0700] {processor.py:186} INFO - Started process (PID=45771) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:00:33.456+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T14:00:33.462+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:00:33.461+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:00:33.541+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:00:33.532+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T14:00:33.543+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:00:33.688+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.275 seconds
[2024-11-20T14:01:04.151+0700] {processor.py:186} INFO - Started process (PID=46029) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:01:04.156+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T14:01:04.179+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:01:04.174+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:01:04.279+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:01:04.261+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T14:01:04.284+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:01:04.747+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.661 seconds
[2024-11-20T14:01:35.275+0700] {processor.py:186} INFO - Started process (PID=46317) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:01:35.280+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T14:01:35.285+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:01:35.284+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:01:35.314+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:01:35.312+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T14:01:35.316+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:01:35.439+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.183 seconds
[2024-11-20T14:02:05.721+0700] {processor.py:186} INFO - Started process (PID=46586) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:02:05.722+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T14:02:05.724+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:02:05.724+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:02:05.735+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:02:05.733+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T14:02:05.736+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:02:05.790+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.078 seconds
[2024-11-20T14:02:36.442+0700] {processor.py:186} INFO - Started process (PID=46849) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:02:36.451+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T14:02:36.474+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:02:36.473+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:02:36.523+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:02:36.511+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T14:02:36.528+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:02:36.676+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.269 seconds
[2024-11-20T14:03:07.745+0700] {processor.py:186} INFO - Started process (PID=47137) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:03:07.747+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T14:03:07.751+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:03:07.750+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:03:07.778+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:03:07.775+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T14:03:07.779+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:03:10.307+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 2.588 seconds
[2024-11-20T14:03:41.129+0700] {processor.py:186} INFO - Started process (PID=47418) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:03:41.130+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T14:03:41.131+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:03:41.131+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:03:41.140+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:03:41.139+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T14:03:41.141+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:03:41.196+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.073 seconds
[2024-11-20T14:04:11.759+0700] {processor.py:186} INFO - Started process (PID=47702) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:04:11.760+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T14:04:11.761+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:04:11.761+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:04:11.774+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:04:11.772+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T14:04:11.775+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:04:11.850+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.103 seconds
[2024-11-20T14:04:49.136+0700] {processor.py:186} INFO - Started process (PID=47978) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:04:49.844+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T14:04:49.857+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:04:49.855+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:04:49.934+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:04:49.923+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T14:04:49.939+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:04:51.659+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 2.621 seconds
[2024-11-20T14:05:22.322+0700] {processor.py:186} INFO - Started process (PID=48297) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:05:22.323+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T14:05:22.327+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:05:22.326+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:05:22.344+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:05:22.342+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T14:05:22.345+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:05:22.407+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.095 seconds
[2024-11-20T14:05:52.972+0700] {processor.py:186} INFO - Started process (PID=48562) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:05:52.973+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T14:05:52.975+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:05:52.974+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:05:53.003+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:05:53.000+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T14:05:53.004+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:05:53.086+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.135 seconds
[2024-11-20T14:06:23.251+0700] {processor.py:186} INFO - Started process (PID=48830) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:06:23.257+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T14:06:23.260+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:06:23.259+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:06:23.290+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:06:23.286+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T14:06:23.291+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:06:23.448+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.215 seconds
[2024-11-20T14:06:54.150+0700] {processor.py:186} INFO - Started process (PID=49138) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:06:54.152+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T14:06:54.155+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:06:54.155+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:06:54.181+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:06:54.176+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T14:06:54.182+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:06:54.402+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.275 seconds
[2024-11-20T14:07:24.598+0700] {processor.py:186} INFO - Started process (PID=49489) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:07:24.601+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T14:07:24.604+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:07:24.604+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:07:24.623+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:07:24.621+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T14:07:24.624+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:07:24.794+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.206 seconds
[2024-11-20T14:07:55.586+0700] {processor.py:186} INFO - Started process (PID=49828) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:07:55.590+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T14:07:55.595+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:07:55.594+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:07:55.644+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:07:55.635+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T14:07:55.647+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:07:55.893+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.325 seconds
[2024-11-20T14:08:26.177+0700] {processor.py:186} INFO - Started process (PID=50098) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:08:26.179+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T14:08:26.182+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:08:26.181+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:08:26.205+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:08:26.202+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T14:08:26.206+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:08:26.302+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.142 seconds
[2024-11-20T14:08:56.513+0700] {processor.py:186} INFO - Started process (PID=50362) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:08:56.516+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T14:08:56.520+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:08:56.519+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:08:56.578+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:08:56.571+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T14:08:56.580+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:08:56.841+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.358 seconds
[2024-11-20T14:09:46.044+0700] {processor.py:186} INFO - Started process (PID=50717) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:09:46.163+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T14:09:46.178+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:09:46.177+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:09:46.258+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:09:46.249+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T14:09:46.261+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:09:47.976+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 1.973 seconds
[2024-11-20T14:10:25.847+0700] {processor.py:186} INFO - Started process (PID=50996) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:10:25.850+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T14:10:25.854+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:10:25.853+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:10:25.897+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:10:25.889+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T14:10:25.899+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:10:26.004+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.182 seconds
[2024-11-20T14:10:56.869+0700] {processor.py:186} INFO - Started process (PID=51331) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:10:56.883+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T14:10:56.887+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:10:56.886+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:10:56.926+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:10:56.922+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T14:10:56.928+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:10:57.061+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.215 seconds
[2024-11-20T14:11:27.480+0700] {processor.py:186} INFO - Started process (PID=51585) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:11:27.496+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T14:11:27.500+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:11:27.499+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:11:27.626+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:11:27.618+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T14:11:27.633+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:11:28.062+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.690 seconds
[2024-11-20T14:11:58.381+0700] {processor.py:186} INFO - Started process (PID=51842) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:11:58.382+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T14:11:58.385+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:11:58.385+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:11:58.429+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:11:58.422+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T14:11:58.447+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:11:58.630+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.279 seconds
[2024-11-20T14:12:28.998+0700] {processor.py:186} INFO - Started process (PID=52096) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:12:29.003+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T14:12:29.008+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:12:29.007+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:12:29.046+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:12:29.043+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T14:12:29.056+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:12:29.197+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.238 seconds
[2024-11-20T14:12:59.406+0700] {processor.py:186} INFO - Started process (PID=52407) to work on /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:12:59.407+0700] {processor.py:914} INFO - Processing file /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py for tasks to queue
[2024-11-20T14:12:59.408+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:12:59.408+0700] {dagbag.py:588} INFO - Filling up the DagBag from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:12:59.423+0700] {logging_mixin.py:190} INFO - [2024-11-20T14:12:59.421+0700] {dagbag.py:387} ERROR - Failed to import: /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
Traceback (most recent call last):
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow-env/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py", line 18, in <module>
    scrape_twitter = SourceFileLoader("scrapping_twitter", scrape_twitter_module_path).load_module().scrape_twitter
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap_external>", line 649, in _check_name_wrapper
  File "<frozen importlib._bootstrap_external>", line 1176, in load_module
  File "<frozen importlib._bootstrap_external>", line 1000, in load_module
  File "<frozen importlib._bootstrap>", line 537, in _load_module_shim
  File "<frozen importlib._bootstrap>", line 966, in _load
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/notebooks/scrapping-twitter.py'
[2024-11-20T14:12:59.425+0700] {processor.py:927} WARNING - No viable dags retrieved from /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py
[2024-11-20T14:12:59.477+0700] {processor.py:208} INFO - Processing /home/fabian4819/REKAYASA DATA/final-exam-etl-pipeline-project/airflow/dags/scrape_twitter_dag.py took 0.081 seconds
